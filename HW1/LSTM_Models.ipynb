{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will be building several varieties of text classifiers.\n",
    "\n",
    "## Goal\n",
    "\n",
    "We ask that you construct the following models in PyTorch:\n",
    "\n",
    "1. A naive Bayes unigram classifer (follow Wang and Manning http://www.aclweb.org/anthology/P/P12/P12-2.pdf#page=118: you should only implement Naive Bayes, not the combined classifer with SVM).\n",
    "2. A logistic regression model over word types (you can implement this as $y = \\sigma(\\sum_i W x_i + b)$) \n",
    "3. A continuous bag-of-word neural network with embeddings (similar to CBOW in Mikolov et al https://arxiv.org/pdf/1301.3781.pdf).\n",
    "4. A simple convolutional neural network (any variant of CNN as described in Kim http://aclweb.org/anthology/D/D14/D14-1181.pdf).\n",
    "5. Your own extensions to these models...\n",
    "\n",
    "Consult the papers provided for hyperparameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "import torch \n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will use of this problem is known as the Stanford Sentiment Treebank (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf). It is a variant of a standard sentiment classification task. For simplicity, we will use the most basic form. Classifying a sentence as positive or negative in sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, `torchtext` requires that we define a mapping from the raw text data to featurized indices. These fields make it easy to map back and forth between readable data and math, which helps for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field(fix_length=56)\n",
    "#TEXT = torchtext.data.Field() \n",
    "# Our labels $y$\n",
    "LABEL = torchtext.data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we input our data. Here we will use the standard SST train split, and tell it the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = torchtext.datasets.SST.splits(\n",
    "    TEXT, LABEL,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this data. It's still in its original form, we can see that each example consists of a label and the original words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to map this data to features, we need to assign an index to each word an label. The function build vocab allows us to do this and provides useful options that we will need in future assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "#this is just the set of stuff \n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to create batches of our training data that can be used for training and validating the model. This function produces 3 iterators that will let us go through the train, val and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(val))\n",
    "print(len(test))\n",
    "print(len(train))\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a single batch from one of these iterators. The library automatically converts the underlying words into indices. It then produces tensors for batches of x and y. In this case it will consist of the number of words of the longest sentence (with padding) followed by the number of batches. We can use the vocabulary dictionary to convert back from these indices to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Size of text batch [max sent length, batch size]\", batch.text.size())\n",
    "print(batch.text[:, 0].data)\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 0].data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "for batch in train_iter: \n",
    "    count += 1 \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly it produces a vector for each of the labels in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of label batch [batch size]\", batch.label.size())\n",
    "print(\"Second in batch\", batch.label[0])\n",
    "print(\"Converted back to string: \", LABEL.vocab.itos[batch.label.data[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the Vocab object can be used to map pretrained word vectors to the indices in the vocabulary. This will be very useful for part 3 and 4 of the problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))\n",
    "\n",
    "print(\"Word embeddings size \", TEXT.vocab.vectors.size())\n",
    "print(\"Word embedding of 'follows', first 10 dim \", TEXT.vocab.vectors[TEXT.vocab.stoi['follows']][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "Using the data given by this iterator, you should construct 4 different torch models that take in batch.text and produce a distribution over labels. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition:  https://www.kaggle.com/c/harvard-cs281-hw1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 16284\n",
      "len(LABEL.vocab) 3\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch.nn as nn \n",
    "\n",
    "# Our input $x$\n",
    "#TEXT = torchtext.data.Field(fix_length=56)\n",
    "TEXT = torchtext.data.Field() \n",
    "# Our labels $y$\n",
    "LABEL = torchtext.data.Field(sequential=False)\n",
    "\n",
    "train, val, test = torchtext.datasets.SST.splits(\n",
    "    TEXT, LABEL,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "#this is just the set of stuff \n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, repeat=False)\n",
    "\n",
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))\n",
    "\n",
    "#Glove embeddings \n",
    "TEXT.vocab.load_vectors(vectors= GloVe(name=\"6B\", dim=\"300\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "/anaconda3/lib/python3.6/site-packages/torch/tensor.py:293: UserWarning: self and other not broadcastable, but have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return self.add(other)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.9612716763005781\n",
      "valid:  0.7931034482758621\n",
      "test:  0.8203296703296703\n"
     ]
    }
   ],
   "source": [
    "def batch_vectorize_init(word_ind, labels, vocab_dim):\n",
    "    num_pos = (labels.data == 2).sum()\n",
    "    num_neg = (labels.data == 1).sum()\n",
    "    pos_out = torch.zeros(vocab_dim)\n",
    "    neg_out = torch.zeros(vocab_dim)\n",
    "    for j in range(word_ind.size(1)):\n",
    "        curr_vec = torch.zeros(vocab_dim)\n",
    "        for i in range(word_ind.size(0)):\n",
    "                curr_vec[int(word_ind[i, j])] = 1\n",
    "        #neg class \n",
    "        if labels.data[j] == 1: \n",
    "            neg_out += curr_vec\n",
    "        elif labels.data[j] == 2: \n",
    "            pos_out += curr_vec\n",
    "        else: \n",
    "            print(\"class not found\")\n",
    "    \n",
    "    return pos_out, neg_out, num_pos, num_neg\n",
    "\n",
    "def batch_vectorize(word_ind, vocab_size):\n",
    "    out = torch.zeros(word_ind.size(1), vocab_size)\n",
    "    for j in range(word_ind.size(1)): \n",
    "        for i in range(word_ind.size(0)):\n",
    "            out[j, int(word_ind[i, j])] = 1 \n",
    "    return out\n",
    "\n",
    "def test_naive(data, vocab_size):\n",
    "    correct = 0.\n",
    "    num_examples = 0.\n",
    "    nll = 0.\n",
    "    for batch in data:\n",
    "        if len(batch.label) == 10: \n",
    "            text, label = batch.text, batch.label\n",
    "            x = batch_vectorize(batch.text, vocab_size)\n",
    "            y_pred = torch.mm(x, R.t()) + b\n",
    "            y_pred_max, y_pred_argmax = torch.max(y_pred, 1) #prediction is the argmax\n",
    "            correct += (y_pred_argmax == label.data - 1).sum() \n",
    "            num_examples += text.size(1) \n",
    "    return correct/num_examples\n",
    "    \n",
    "n_pos = 0 \n",
    "n_neg = 0 \n",
    "alpha = 0.5 \n",
    "p = torch.zeros(len(TEXT.vocab))\n",
    "q = torch.zeros(len(TEXT.vocab))\n",
    "for batch in train_iter:\n",
    "    text, label = batch.text, batch.label\n",
    "    pos_vec, neg_vec, curr_pos, curr_neg = batch_vectorize_init(batch.text, batch.label, len(TEXT.vocab))\n",
    "    n_pos += curr_pos\n",
    "    n_neg += curr_neg\n",
    "    p += pos_vec \n",
    "    q += neg_vec \n",
    "n_pos_vec = torch.log(torch.Tensor([n_pos/(n_pos+n_neg)])).repeat(10)\n",
    "n_neg_vec = torch.log(torch.Tensor([n_neg/(n_pos+n_neg)])).repeat(10)\n",
    "b = torch.cat((n_neg_vec, n_pos_vec)).view(2, -1)\n",
    "p += alpha \n",
    "q += alpha \n",
    "R = torch.log(torch.cat((q/torch.abs(q).sum(), p/torch.abs(p).sum())).view(2, -1))\n",
    "\n",
    "print(\"train: \", test_naive(train_iter, len(TEXT.vocab)))\n",
    "print(\"valid: \", test_naive(val_iter, len(TEXT.vocab)))\n",
    "print(\"test: \", test_naive(test_iter, len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_Unigram(\n",
      "  (embed): Embedding(16284, 16284)\n",
      "  (linear): Linear(in_features=16284, out_features=2)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (logsoftmax): LogSoftmax()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training performance after epoch 1: NLL: 1.2077, Accuracy: 0.6874\n",
      "Validation performance after epoch 1: NLL: 1.2612, Accuracy: 0.6972\n",
      "Training performance after epoch 2: NLL: 1.1492, Accuracy: 0.7475\n",
      "Validation performance after epoch 2: NLL: 1.2271, Accuracy: 0.7282\n",
      "Training performance after epoch 3: NLL: 1.1235, Accuracy: 0.7754\n",
      "Validation performance after epoch 3: NLL: 1.1963, Accuracy: 0.7626\n",
      "Training performance after epoch 4: NLL: 1.0818, Accuracy: 0.7880\n",
      "Validation performance after epoch 4: NLL: 1.1836, Accuracy: 0.7259\n",
      "Training performance after epoch 5: NLL: 1.0539, Accuracy: 0.8143\n",
      "Validation performance after epoch 5: NLL: 1.1667, Accuracy: 0.7534\n",
      "Training performance after epoch 6: NLL: 1.0334, Accuracy: 0.8214\n",
      "Validation performance after epoch 6: NLL: 1.1602, Accuracy: 0.7511\n",
      "Training performance after epoch 7: NLL: 1.0144, Accuracy: 0.8448\n",
      "Validation performance after epoch 7: NLL: 1.1618, Accuracy: 0.7546\n",
      "Training performance after epoch 8: NLL: 1.0005, Accuracy: 0.8432\n",
      "Validation performance after epoch 8: NLL: 1.1569, Accuracy: 0.7443\n",
      "Training performance after epoch 9: NLL: 0.9838, Accuracy: 0.8616\n",
      "Validation performance after epoch 9: NLL: 1.1504, Accuracy: 0.7569\n",
      "Training performance after epoch 10: NLL: 0.9708, Accuracy: 0.8652\n",
      "Validation performance after epoch 10: NLL: 1.1351, Accuracy: 0.7638\n",
      "Training performance after epoch 11: NLL: 0.9574, Accuracy: 0.8847\n",
      "Validation performance after epoch 11: NLL: 1.1337, Accuracy: 0.7729\n",
      "Training performance after epoch 12: NLL: 0.9475, Accuracy: 0.8842\n",
      "Validation performance after epoch 12: NLL: 1.1304, Accuracy: 0.7661\n",
      "Training performance after epoch 13: NLL: 0.9363, Accuracy: 0.8896\n",
      "Validation performance after epoch 13: NLL: 1.1325, Accuracy: 0.7672\n",
      "Training performance after epoch 14: NLL: 0.9282, Accuracy: 0.8905\n",
      "Validation performance after epoch 14: NLL: 1.1241, Accuracy: 0.7638\n",
      "Training performance after epoch 15: NLL: 0.9200, Accuracy: 0.8957\n",
      "Validation performance after epoch 15: NLL: 1.1211, Accuracy: 0.7615\n",
      "Training performance after epoch 16: NLL: 0.9201, Accuracy: 0.8838\n",
      "Validation performance after epoch 16: NLL: 1.1251, Accuracy: 0.7615\n",
      "Training performance after epoch 17: NLL: 0.9037, Accuracy: 0.9071\n",
      "Validation performance after epoch 17: NLL: 1.1201, Accuracy: 0.7718\n",
      "Training performance after epoch 18: NLL: 0.8971, Accuracy: 0.9087\n",
      "Validation performance after epoch 18: NLL: 1.1173, Accuracy: 0.7672\n",
      "Training performance after epoch 19: NLL: 0.8920, Accuracy: 0.9113\n",
      "Validation performance after epoch 19: NLL: 1.1158, Accuracy: 0.7683\n",
      "Training performance after epoch 20: NLL: 0.8854, Accuracy: 0.9163\n",
      "Validation performance after epoch 20: NLL: 1.1158, Accuracy: 0.7706\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, data):\n",
    "    correct = 0.\n",
    "    num_examples = 0.\n",
    "    nll = 0.\n",
    "    for batch in data:\n",
    "        text = batch.text\n",
    "        label = batch.label\n",
    "        y_pred = model(text)\n",
    "        nll_batch = criterion(y_pred, label-1)\n",
    "        nll += nll_batch.data[0] * text.size(0) #by default NLL is averaged over each batch\n",
    "        y_pred_max, y_pred_argmax = torch.max(y_pred, 1) #prediction is the argmax\n",
    "        correct += (y_pred_argmax.data == label.data-1).sum() \n",
    "        num_examples += text.size(1) \n",
    "    return nll/num_examples, correct/num_examples\n",
    "\n",
    "class LR_Unigram(nn.Module): \n",
    "    def __init__(self, vocab, output_dim=2):\n",
    "        super(LR_Unigram, self).__init__()\n",
    "        self.embed = nn.Embedding(len(vocab), len(vocab))\n",
    "        self.embed.weight.data = torch.eye(len(vocab))\n",
    "        self.embed.weight.requires_grad = False\n",
    "        self.input_dim = len(vocab)\n",
    "        self.linear = nn.Linear(len(vocab), output_dim, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x_embed = self.embed(x.t())\n",
    "        x_flatten = torch.sum(x_embed, dim=1)\n",
    "        out = self.linear(x_flatten)\n",
    "        out = self.sigmoid(out)\n",
    "        return self.logsoftmax(out)\n",
    "\n",
    "lr_model = LR_Unigram(TEXT.vocab)\n",
    "print(lr_model)\n",
    "criterion = nn.NLLLoss()\n",
    "parameters = filter(lambda p: p.requires_grad, lr_model.parameters())\n",
    "optim = torch.optim.SGD(parameters, lr = 0.5)\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    for batch in train_iter:\n",
    "        optim.zero_grad()\n",
    "        #text = torch.autograd.Variable(batch_index_to_vec(batch.text))\n",
    "        text = batch.text \n",
    "        label = batch.label\n",
    "        y_pred = lr_model(text)\n",
    "        nll_batch = criterion(y_pred, label - 1)    \n",
    "        nll_batch.backward()\n",
    "        optim.step()\n",
    "    nll_train, accuracy_train = test_model(lr_model, train_iter)\n",
    "    nll_val, accuracy_val = test_model(lr_model, val_iter)\n",
    "    print('Training performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_train, accuracy_train))\n",
    "    print('Validation performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_val, accuracy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test performance after epoch 20: NLL: 1.0818, Accuracy: 0.7908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "nll_test, accuracy_test = test_model(lr_model, test_iter)\n",
    "print('Test performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3: CBOW \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.twitter.27B.zip: 1.52GB [02:15, 11.2MB/s]                               \n",
      "100%|██████████| 1193514/1193514 [01:36<00:00, 12338.31it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT.vocab.load_vectors(vectors=GloVe(name=\"twitter.27B\", dim=\"200\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (embed): Embedding(16284, 200)\n",
      "  (linear): Linear(in_features=200, out_features=2)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (logsoftmax): LogSoftmax()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training performance after epoch 1: NLL: 1.6389, Accuracy: 0.4783\n",
      "Validation performance after epoch 1: NLL: 1.6947, Accuracy: 0.4908\n",
      "Training performance after epoch 2: NLL: 1.6378, Accuracy: 0.4783\n",
      "Validation performance after epoch 2: NLL: 1.6947, Accuracy: 0.4908\n",
      "Training performance after epoch 3: NLL: 1.3542, Accuracy: 0.5160\n",
      "Validation performance after epoch 3: NLL: 1.4012, Accuracy: 0.5195\n",
      "Training performance after epoch 4: NLL: 1.3546, Accuracy: 0.5267\n",
      "Validation performance after epoch 4: NLL: 1.4014, Accuracy: 0.5264\n",
      "Training performance after epoch 5: NLL: 1.3540, Accuracy: 0.5269\n",
      "Validation performance after epoch 5: NLL: 1.4014, Accuracy: 0.5264\n",
      "Training performance after epoch 6: NLL: 1.6373, Accuracy: 0.4783\n",
      "Validation performance after epoch 6: NLL: 1.6947, Accuracy: 0.4908\n",
      "Training performance after epoch 7: NLL: 1.3535, Accuracy: 0.5299\n",
      "Validation performance after epoch 7: NLL: 1.4014, Accuracy: 0.5321\n",
      "Training performance after epoch 8: NLL: 1.6370, Accuracy: 0.4785\n",
      "Validation performance after epoch 8: NLL: 1.6944, Accuracy: 0.4908\n",
      "Training performance after epoch 9: NLL: 1.3520, Accuracy: 0.5348\n",
      "Validation performance after epoch 9: NLL: 1.4003, Accuracy: 0.5390\n",
      "Training performance after epoch 10: NLL: 1.3504, Accuracy: 0.5409\n",
      "Validation performance after epoch 10: NLL: 1.3975, Accuracy: 0.5447\n",
      "Training performance after epoch 11: NLL: 1.3496, Accuracy: 0.5419\n",
      "Validation performance after epoch 11: NLL: 1.3971, Accuracy: 0.5550\n",
      "Training performance after epoch 12: NLL: 1.3465, Accuracy: 0.5600\n",
      "Validation performance after epoch 12: NLL: 1.3937, Accuracy: 0.5711\n",
      "Training performance after epoch 13: NLL: 1.3487, Accuracy: 0.5532\n",
      "Validation performance after epoch 13: NLL: 1.3966, Accuracy: 0.5550\n",
      "Training performance after epoch 14: NLL: 1.6336, Accuracy: 0.4790\n",
      "Validation performance after epoch 14: NLL: 1.6922, Accuracy: 0.4908\n",
      "Training performance after epoch 15: NLL: 1.3489, Accuracy: 0.5682\n",
      "Validation performance after epoch 15: NLL: 1.3958, Accuracy: 0.5700\n",
      "Training performance after epoch 16: NLL: 1.2841, Accuracy: 0.6731\n",
      "Validation performance after epoch 16: NLL: 1.3434, Accuracy: 0.6904\n",
      "Training performance after epoch 17: NLL: 1.3320, Accuracy: 0.6306\n",
      "Validation performance after epoch 17: NLL: 1.3802, Accuracy: 0.6261\n",
      "Training performance after epoch 18: NLL: 1.3427, Accuracy: 0.5913\n",
      "Validation performance after epoch 18: NLL: 1.3893, Accuracy: 0.5849\n",
      "Training performance after epoch 19: NLL: 1.2532, Accuracy: 0.7116\n",
      "Validation performance after epoch 19: NLL: 1.2801, Accuracy: 0.7339\n",
      "Training performance after epoch 20: NLL: 1.4092, Accuracy: 0.5854\n",
      "Validation performance after epoch 20: NLL: 1.4344, Accuracy: 0.6067\n",
      "Training performance after epoch 21: NLL: 1.2930, Accuracy: 0.6828\n",
      "Validation performance after epoch 21: NLL: 1.3490, Accuracy: 0.6732\n",
      "Training performance after epoch 22: NLL: 1.2815, Accuracy: 0.6949\n",
      "Validation performance after epoch 22: NLL: 1.3375, Accuracy: 0.6972\n",
      "Training performance after epoch 23: NLL: 1.2658, Accuracy: 0.6926\n",
      "Validation performance after epoch 23: NLL: 1.2932, Accuracy: 0.7282\n",
      "Training performance after epoch 24: NLL: 1.3506, Accuracy: 0.5486\n",
      "Validation performance after epoch 24: NLL: 1.4001, Accuracy: 0.5310\n",
      "Training performance after epoch 25: NLL: 1.3208, Accuracy: 0.6377\n",
      "Validation performance after epoch 25: NLL: 1.3724, Accuracy: 0.6261\n",
      "Training performance after epoch 26: NLL: 1.2774, Accuracy: 0.6981\n",
      "Validation performance after epoch 26: NLL: 1.3360, Accuracy: 0.6915\n",
      "Training performance after epoch 27: NLL: 1.2562, Accuracy: 0.7090\n",
      "Validation performance after epoch 27: NLL: 1.2944, Accuracy: 0.7328\n",
      "Training performance after epoch 28: NLL: 1.2524, Accuracy: 0.7227\n",
      "Validation performance after epoch 28: NLL: 1.3157, Accuracy: 0.7202\n",
      "Training performance after epoch 29: NLL: 1.3035, Accuracy: 0.6584\n",
      "Validation performance after epoch 29: NLL: 1.3605, Accuracy: 0.6479\n",
      "Training performance after epoch 30: NLL: 1.3340, Accuracy: 0.6065\n",
      "Validation performance after epoch 30: NLL: 1.3820, Accuracy: 0.5929\n",
      "Training performance after epoch 31: NLL: 1.3280, Accuracy: 0.6197\n",
      "Validation performance after epoch 31: NLL: 1.3769, Accuracy: 0.6032\n",
      "Training performance after epoch 32: NLL: 1.2416, Accuracy: 0.7309\n",
      "Validation performance after epoch 32: NLL: 1.2904, Accuracy: 0.7454\n",
      "Training performance after epoch 33: NLL: 1.3261, Accuracy: 0.6269\n",
      "Validation performance after epoch 33: NLL: 1.3745, Accuracy: 0.5998\n",
      "Training performance after epoch 34: NLL: 1.3453, Accuracy: 0.5711\n",
      "Validation performance after epoch 34: NLL: 1.3920, Accuracy: 0.5528\n",
      "Training performance after epoch 35: NLL: 1.2421, Accuracy: 0.7436\n",
      "Validation performance after epoch 35: NLL: 1.3006, Accuracy: 0.7420\n",
      "Training performance after epoch 36: NLL: 1.3077, Accuracy: 0.6558\n",
      "Validation performance after epoch 36: NLL: 1.3642, Accuracy: 0.6365\n",
      "Training performance after epoch 37: NLL: 1.2783, Accuracy: 0.6965\n",
      "Validation performance after epoch 37: NLL: 1.3354, Accuracy: 0.6686\n",
      "Training performance after epoch 38: NLL: 1.2722, Accuracy: 0.7056\n",
      "Validation performance after epoch 38: NLL: 1.3329, Accuracy: 0.6755\n",
      "Training performance after epoch 39: NLL: 1.3436, Accuracy: 0.6707\n",
      "Validation performance after epoch 39: NLL: 1.3904, Accuracy: 0.6892\n",
      "Training performance after epoch 40: NLL: 1.3212, Accuracy: 0.6341\n",
      "Validation performance after epoch 40: NLL: 1.3737, Accuracy: 0.6032\n"
     ]
    }
   ],
   "source": [
    "def test_cbow(model, data):\n",
    "    correct = 0.\n",
    "    num_examples = 0.\n",
    "    nll = 0.\n",
    "    for batch in data:\n",
    "        text = batch.text\n",
    "        label = batch.label\n",
    "        y_pred = model(text)\n",
    "        nll_batch = criterion(y_pred, label-1)\n",
    "        nll += nll_batch.data[0] * text.size(0) #by default NLL is averaged over each batch\n",
    "        y_pred_max, y_pred_argmax = torch.max(y_pred, 1) #prediction is the argmax\n",
    "        correct += (y_pred_argmax.data == label.data-1).sum() \n",
    "        num_examples += text.size(1) \n",
    "    return nll/num_examples, correct/num_examples\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, embedding_dim, output_dim=2):\n",
    "        super(CBOW, self).__init__()\n",
    "        #linear classifier \n",
    "        self.embed = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.embed.weight.data.copy_(vocab.vectors)\n",
    "        self.embed.weight.requires_grad = False\n",
    "        self.linear = nn.Linear(embedding_dim, output_dim, bias=True)\n",
    "        #activation function \n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embed(x.t())\n",
    "        x_flatten = torch.sum(x_embed, dim=1)\n",
    "        out = self.linear(x_flatten)\n",
    "        out = self.sigmoid(out)\n",
    "        return self.logsoftmax(out)\n",
    "    \n",
    "cbow_model = CBOW(TEXT.vocab, embedding_dim=200)\n",
    "print(cbow_model)\n",
    "criterion = nn.NLLLoss()\n",
    "parameters = filter(lambda p: p.requires_grad, cbow_model.parameters())\n",
    "optim = torch.optim.SGD(parameters, lr = 0.5)\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    for batch in train_iter:\n",
    "        optim.zero_grad()\n",
    "        #text = torch.autograd.Variable(batch_index_to_vec(batch.text))\n",
    "        text = batch.text \n",
    "        label = batch.label\n",
    "        y_pred = cbow_model(text)\n",
    "        nll_batch = criterion(y_pred, label - 1)    \n",
    "        nll_batch.backward()\n",
    "        optim.step()\n",
    "    nll_train, accuracy_train = test_cbow(cbow_model, train_iter)\n",
    "    nll_val, accuracy_val = test_cbow(cbow_model, val_iter)\n",
    "    print('Training performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_train, accuracy_train))\n",
    "    print('Validation performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_val, accuracy_val))\n",
    "\n",
    "torch.save(cbow_model.state_dict(), 'cbow_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test performance after epoch 40: NLL: 1.3336, Accuracy: 0.6019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "nll_test, accuracy_test = test_cbow(cbow_model, test_iter)\n",
    "print('Test performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 16284\n",
      "len(LABEL.vocab) 3\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from random import shuffle\n",
    "\n",
    "# Our input $x$\n",
    "seq_length = 56\n",
    "TEXT = torchtext.data.Field(fix_length=seq_length)\n",
    "\n",
    "# Our labels $y$\n",
    "LABEL = torchtext.data.Field(sequential=False)\n",
    "\n",
    "train, val, test = torchtext.datasets.SST.splits(\n",
    "    TEXT, LABEL,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "train_2, val, test = torchtext.datasets.SST.splits(\n",
    "    TEXT, LABEL,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "#this is just the set of stuff \n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))\n",
    "\n",
    "for i in range(len(train_2)): \n",
    "    shuffle(train_2[i].text) \n",
    "    \n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train + train_2, val, test), batch_size=10, device=-1, repeat=False, shuffle=True)\n",
    "\n",
    "#Glove embeddings \n",
    "TEXT.vocab.load_vectors(vectors= GloVe(name=\"6B\", dim=\"300\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) <class 'torchtext.datasets.sst.SST'>\n",
      "vars(train[0]) ['new', 'that', 'Steven', 'he', 'splash', 'Damme', 'make', 'a', 'greater', 'the', 'destined', 'Century', 'or', 'Van', 'Schwarzenegger', 'and', 'going', 'Conan', \"''\", 'Rock', 'Segal', '``', 'than', 'Jean-Claud', \"'s\", 'even', 'to', '.', '21st', ',', 'The', 'be', 'Arnold', 'to', 'is', \"'s\"]\n",
      "vars(train[0]) None\n",
      "vars(train[0]) ['to', 'or', 'that', 'Segal', 'Van', 'even', 'be', \"'s\", ',', '``', 'Century', 'the', 'Steven', 'Conan', 'Schwarzenegger', 'destined', \"'s\", 'make', 'is', 'new', 'a', 'to', 'Rock', 'he', 'and', 'The', 'Damme', '.', 'Jean-Claud', 'splash', '21st', 'Arnold', 'greater', 'going', 'than', \"''\"]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cnn(model, data):\n",
    "    correct = 0.\n",
    "    num_examples = 0.\n",
    "    nll = 0.\n",
    "    for batch in data:\n",
    "        text = batch.text\n",
    "        label = batch.label\n",
    "        y_pred = model(text)\n",
    "        nll_batch = criterion(y_pred, label - 1)\n",
    "        nll += nll_batch.data[0] * text.size(0) #by default NLL is averaged over each batch\n",
    "        y_pred_max, y_pred_argmax = torch.max(y_pred, 1) #prediction is the argmax\n",
    "        correct += (y_pred_argmax.data == label.data - 1).sum() \n",
    "        num_examples += text.size(1)\n",
    "    return nll/num_examples, correct/num_examples\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, seq_len, embedding_dim, output_dim=2):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embed = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.embed.weight.data.copy_(vocab.vectors)\n",
    "        map_size = 100\n",
    "        filter_w = embedding_dim\n",
    "        filter_h = 4\n",
    "        self.conv1 = nn.Conv2d(1, map_size, (filter_h, filter_w))\n",
    "        #self.bn = nn.BatchNorm2d(map_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU() \n",
    "        pool_h = seq_len-filter_h + 1 \n",
    "        pool_w = 1 \n",
    "        self.pooling = nn.MaxPool2d((pool_h, pool_w))\n",
    "        #self.embed.weight.requires_grad = False\n",
    "        self.fc = nn.Linear(map_size, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # here x is batch size x length of post X embedding dim \n",
    "        #print(x.shape)\n",
    "        x_embed = self.embed(x.t())\n",
    "        #print(x_embed.shape)\n",
    "        fc = self.conv1(x_embed.unsqueeze(1))\n",
    "        #print(fc.shape)\n",
    "        #pool = self.pooling(self.bn(fc))\n",
    "        pool = self.pooling(self.dropout(fc))\n",
    "        relu = self.relu(pool)\n",
    "        #print(pool.shape)\n",
    "        out = self.fc(pool.view(x_embed.size(0), -1))\n",
    "        out = self.sigmoid(out)\n",
    "        return self.logsoftmax(out)\n",
    "\n",
    "cnn_model = CNN(TEXT.vocab, seq_len=seq_length, embedding_dim=300)\n",
    "criterion = nn.NLLLoss()\n",
    "parameters = filter(lambda p: p.requires_grad, cnn_model.parameters())\n",
    "optim = torch.optim.SGD(parameters, lr = 0.1)\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    for batch in train_iter:\n",
    "        optim.zero_grad()\n",
    "        text = batch.text\n",
    "        label = batch.label\n",
    "        y_pred = cnn_model(text)\n",
    "        nll_batch = criterion(y_pred, label-1)    \n",
    "        nll_batch.backward()\n",
    "        optim.step()\n",
    "    nll_train, accuracy_train = test_cnn(cnn_model, train_iter)\n",
    "    nll_val, accuracy_val = test_cnn(cnn_model, val_iter)\n",
    "    print('Training performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_train, accuracy_train))\n",
    "    print('Validation performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_val, accuracy_val))\n",
    "\n",
    "torch.save(cnn_model.state_dict(), 'cnn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_test, accuracy_test = test_cnn(cnn_model, test_iter)\n",
    "print('Test performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test performance after epoch 1: NLL: 381.0838, Accuracy: 0.5022\n",
      "Training performance after epoch 1: NLL: 360.7707, Accuracy: 0.5189\n",
      "Validation performance after epoch 1: NLL: 368.4312, Accuracy: 0.5149\n",
      "Test performance after epoch 2: NLL: 1207.8478, Accuracy: 0.5011\n",
      "Training performance after epoch 2: NLL: 1260.8997, Accuracy: 0.4783\n",
      "Validation performance after epoch 2: NLL: 1218.2925, Accuracy: 0.4920\n",
      "Test performance after epoch 3: NLL: 478.3004, Accuracy: 0.5011\n",
      "Training performance after epoch 3: NLL: 502.2271, Accuracy: 0.4789\n",
      "Validation performance after epoch 3: NLL: 486.3800, Accuracy: 0.4908\n",
      "Test performance after epoch 4: NLL: 61.6009, Accuracy: 0.5154\n",
      "Training performance after epoch 4: NLL: 59.1166, Accuracy: 0.5451\n",
      "Validation performance after epoch 4: NLL: 55.4619, Accuracy: 0.5402\n",
      "Test performance after epoch 5: NLL: 17.0739, Accuracy: 0.5489\n",
      "Training performance after epoch 5: NLL: 17.7876, Accuracy: 0.5426\n",
      "Validation performance after epoch 5: NLL: 16.5534, Accuracy: 0.5517\n",
      "Test performance after epoch 6: NLL: 1090.7890, Accuracy: 0.4989\n",
      "Training performance after epoch 6: NLL: 1042.3101, Accuracy: 0.5217\n",
      "Validation performance after epoch 6: NLL: 1081.6476, Accuracy: 0.5080\n"
     ]
    }
   ],
   "source": [
    "def test_lstm(model, data):\n",
    "    correct = 0.\n",
    "    num_examples = 0.\n",
    "    nll = 0.\n",
    "    for batch in data:\n",
    "        if batch.text.size(1) == 10: \n",
    "            text = batch.text\n",
    "            label = batch.label\n",
    "            model.hidden = model.init_hidden()\n",
    "            y_pred = model(text)\n",
    "            nll_batch = criterion(y_pred, label - 1)\n",
    "            nll += nll_batch.data[0] * text.size(0) #by default NLL is averaged over each batch\n",
    "            y_pred_max, y_pred_argmax = torch.max(y_pred, 1) #prediction is the argmax\n",
    "            correct += (y_pred_argmax.data == label.data - 1).sum() \n",
    "            num_examples += text.size(1)\n",
    "    return nll/num_examples, correct/num_examples\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, vocab, batch_size, seq_len, embedding_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embed = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.embed.weight.data.copy_(vocab.vectors)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim + embedding_dim, 2)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)),\n",
    "               torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embed(sentence.t())\n",
    "        x = torch.t(embeds)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        cat_layer = torch.cat((torch.sum(embeds, dim=1), torch.mean(lstm_out, dim=0)), dim=1)\n",
    "        y  = self.fc(cat_layer) \n",
    "        out = F.log_softmax(y, dim=1)\n",
    "        return out\n",
    "\n",
    "lstm_model = LSTM(hidden_dim=100, vocab=TEXT.vocab, batch_size=10, seq_len=56, embedding_dim=300)\n",
    "criterion = nn.NLLLoss()\n",
    "parameters = filter(lambda p: p.requires_grad, lstm_model.parameters())\n",
    "optim = torch.optim.SGD(parameters, lr = 0.05, weight_decay=0.0001)\n",
    "num_epochs = 20\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for batch in train_iter:\n",
    "        optim.zero_grad()\n",
    "        lstm_model.hidden = lstm_model.init_hidden()\n",
    "        #text = torch.cat((batch.text[torch.randperm(15), :], batch.text[15:, :])) \n",
    "        text = batch.text\n",
    "        label = batch.label\n",
    "        y_pred = lstm_model(text)\n",
    "        nll_batch = criterion(y_pred, label-1) \n",
    "        nll_batch.backward()\n",
    "        optim.step()\n",
    "    nll_train, accuracy_train = test_lstm(lstm_model, train_iter)\n",
    "    nll_val, accuracy_val = test_lstm(lstm_model, val_iter)\n",
    "    nll_test, accuracy_test = test_lstm(lstm_model, test_iter)\n",
    "    print('Test performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_test, accuracy_test))\n",
    "    print('Training performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_train, accuracy_train))\n",
    "    print('Validation performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_val, accuracy_val))\n",
    "\n",
    "torch.save(lstm_model.state_dict(), 'lstm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test performance after epoch 20: NLL: 4.3825, Accuracy: 0.8038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "nll_test, accuracy_test = test_lstm(lstm_model, test_iter)\n",
    "print('Test performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test performance after epoch 17: NLL: 5.7222, Accuracy: 0.7984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training performance after epoch 1: NLL: 3.8572, Accuracy: 0.6014\n",
      "Validation performance after epoch 1: NLL: 3.8562, Accuracy: 0.6161\n",
      "Test performance after epoch 1: NLL: 3.8573, Accuracy: 0.6148\n",
      "Training performance after epoch 2: NLL: 3.8103, Accuracy: 0.5247\n",
      "Validation performance after epoch 2: NLL: 3.8217, Accuracy: 0.5092\n",
      "Test performance after epoch 2: NLL: 3.8327, Accuracy: 0.4995\n",
      "Training performance after epoch 3: NLL: 3.7097, Accuracy: 0.6473\n",
      "Validation performance after epoch 3: NLL: 3.6879, Accuracy: 0.6724\n",
      "Test performance after epoch 3: NLL: 3.6933, Accuracy: 0.6533\n",
      "Training performance after epoch 4: NLL: 3.4764, Accuracy: 0.6384\n",
      "Validation performance after epoch 4: NLL: 3.5845, Accuracy: 0.6333\n",
      "Test performance after epoch 4: NLL: 3.6006, Accuracy: 0.6104\n",
      "Training performance after epoch 5: NLL: 3.1567, Accuracy: 0.7425\n",
      "Validation performance after epoch 5: NLL: 3.3123, Accuracy: 0.7379\n",
      "Test performance after epoch 5: NLL: 3.3116, Accuracy: 0.7214\n",
      "Training performance after epoch 6: NLL: 2.8053, Accuracy: 0.7695\n",
      "Validation performance after epoch 6: NLL: 2.9518, Accuracy: 0.7563\n",
      "Test performance after epoch 6: NLL: 2.9180, Accuracy: 0.7665\n",
      "Training performance after epoch 7: NLL: 2.7802, Accuracy: 0.7762\n",
      "Validation performance after epoch 7: NLL: 2.8361, Accuracy: 0.7713\n",
      "Test performance after epoch 7: NLL: 2.8399, Accuracy: 0.7659\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-5bfedcf1ad7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mnll_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mnll_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mnll_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbilstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_lstm(model, data):\n",
    "    correct = 0.\n",
    "    num_examples = 0.\n",
    "    nll = 0.\n",
    "    for batch in data:\n",
    "        if batch.text.size(1) == 10: \n",
    "            text = batch.text\n",
    "            label = batch.label\n",
    "            model.hidden = model.init_hidden()\n",
    "            y_pred = model(text)\n",
    "            nll_batch = criterion(y_pred, label - 1)\n",
    "            nll += nll_batch.data[0] * text.size(0) #by default NLL is averaged over each batch\n",
    "            y_pred_max, y_pred_argmax = torch.max(y_pred, 1) #prediction is the argmax\n",
    "            correct += (y_pred_argmax.data == label.data - 1).sum() \n",
    "            num_examples += text.size(1)\n",
    "    return nll/num_examples, correct/num_examples\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, vocab, batch_size, seq_len, embedding_dim):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embed = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.embed.weight.data.copy_(vocab.vectors)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=0.5, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2 + embedding_dim, 2)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.autograd.Variable(torch.zeros(2, self.batch_size, self.hidden_dim)),\n",
    "               torch.autograd.Variable(torch.zeros(2, self.batch_size, self.hidden_dim)))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embed(sentence.t())\n",
    "        x = torch.t(embeds)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        cat_layer = torch.cat((torch.mean(embeds), torch.mean(lstm_out, dim=0)))\n",
    "        y  = self.fc(cat_layer)\n",
    "        out = F.log_softmax(y, dim=1)\n",
    "        return out\n",
    "\n",
    "bilstm_model = BiLSTM(hidden_dim=100, vocab=TEXT.vocab, batch_size=10, seq_len=56, embedding_dim=300)\n",
    "criterion = nn.NLLLoss()\n",
    "parameters = filter(lambda p: p.requires_grad, bilstm_model.parameters())\n",
    "optim = torch.optim.SGD(parameters, lr = 0.05, weight_decay=0.0001)\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    for batch in train_iter:\n",
    "        optim.zero_grad()\n",
    "        bilstm_model.hidden = bilstm_model.init_hidden()\n",
    "        text = batch.text\n",
    "        label = batch.label\n",
    "        y_pred = bilstm_model(text)\n",
    "        nll_batch = criterion(y_pred, label-1) \n",
    "        nll_batch.backward()\n",
    "        optim.step()\n",
    "    nll_train, accuracy_train = test_lstm(bilstm_model, train_iter)\n",
    "    nll_val, accuracy_val = test_lstm(bilstm_model, val_iter)\n",
    "    print('Training performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_train, accuracy_train))\n",
    "    print('Validation performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_val, accuracy_val))\n",
    "    nll_test, accuracy_test = test_lstm(bilstm_model, test_iter)\n",
    "    print('Test performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_test, accuracy_test))\n",
    "\n",
    "torch.save(bilstm_model.state_dict(), 'bilstm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attack:\n",
    "    def __init__(self, model):\n",
    "        self.net = model\n",
    "        self.optimizer = optim.SGD(params=[self.net.r], lr=0.008)\n",
    "\n",
    "    def attack(self, x, y_true, y_target, regularization=None):\n",
    "        _x = x\n",
    "        _y_target = Variable(torch.LongTensor([y_target]))\n",
    "\n",
    "        # Reset value of r word perturbations\n",
    "        self.net.r.data = torch.zeros(1,56*300) \n",
    "\n",
    "        y_pred =  np.argmax(self.net(_x).data.numpy())\n",
    "        incorrect_classify = False\n",
    "        if y_true != y_pred:\n",
    "            incorrect_classify = True\n",
    "\n",
    "        # Optimization Loop \n",
    "        for iteration in range(1000):\n",
    "\n",
    "            self.optimizer.zero_grad() \n",
    "            outputs = self.net(_x)\n",
    "            xent_loss = self.softmaxwithxent(outputs, _y_target) \n",
    "            adv_loss = xent_loss + torch.mean(torch.abs(self.net.r))\n",
    "            adv_loss.backward() \n",
    "            self.optimizer.step() \n",
    "\n",
    "            # keep optimizing Until classif_op == _y_target\n",
    "            y_pred_adversarial = np.argmax(self.net(_x).data.numpy())\n",
    "            if y_pred_adversarial == y_target:\n",
    "                break \n",
    "\n",
    "        if iteration == 999:\n",
    "            print \"Warning: optimization loop ran for 1000 iterations. The result may not be correct\"\n",
    "\n",
    "        return self.net.r.data.numpy(), y_pred, y_pred_adversarial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CNN(TEXT.vocab, embedding_dim=300)\n",
    "print(net)\n",
    "SoftmaxWithXent = nn.CrossEntropyLoss()\n",
    "\n",
    "# OPTIMIZE FOR \"r\" \n",
    "optimizer = optim.SGD(params=[net.r], lr=0.008)\n",
    "\n",
    "\n",
    "for example in test:\n",
    "    _x = example.text \n",
    "    _y_target = (example.label - 1) ^ (example.label - 1)\n",
    "    \n",
    "    # Reset value of r \n",
    "    net.r.data = torch.zeros(1, 56) \n",
    "\n",
    "    # Classification before Adv \n",
    "    y_pred =  np.argmax(net(Variable(_x)).data.numpy())\n",
    "    y_preds.append(y_pred)\n",
    "    \n",
    "    print \"Y_TRUE: {} | Y_PRED: {}\".format(_y_true, y_pred)\n",
    "    if _y_true != y_pred:\n",
    "        print \"WARNING: IMAGE WAS NOT CLASSIFIED CORRECTLY\"\n",
    "\n",
    "    # Optimization Loop \n",
    "    tqd_loop = trange(1000)\n",
    "    for iteration in tqd_loop:\n",
    "\n",
    "        x,y = _x, _y_target\n",
    "        optimizer.zero_grad() \n",
    "        outputs = net(x)\n",
    "        xent_loss = SoftmaxWithXent(outputs, y) \n",
    "        adv_loss  = xent_loss + torch.mean(torch.pow(net.r,2))\n",
    "\n",
    "        adv_loss.backward() \n",
    "        # xent_loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        # print stats \n",
    "        classif_op = np.argmax(net(Variable(_x)).data.numpy())\n",
    "        tqd_loop.set_description(\"xent Loss: {} classif: {}\".format(xent_loss.data.numpy(), classif_op))\n",
    "\n",
    "        # keep optimizing Until classif_op == _y_target\n",
    "        if classif_op == _y_target.numpy()[0]:\n",
    "            tqd_loop.close()\n",
    "            break \n",
    "\n",
    "    # save adv_image and noise to list \n",
    "    noise.append(net.r.data.numpy())\n",
    "    print \"After Optimization Image is classified as: \"\n",
    "    print np.argmax(net(Variable(_x)).data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Test Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_iter):\n",
    "    \"All models should be able to be run with following command.\"\n",
    "    upload = []\n",
    "    # Update: for kaggle the bucket iterator needs to have batch_size 10\n",
    "    #test_iter = torchtext.data.BucketIterator(test, train=False, batch_size=10)\n",
    "    for batch in test_iter:\n",
    "        # Your prediction data here (don't cheat!)\n",
    "        probs = model(batch.text)\n",
    "        _, argmax = probs.max(1)\n",
    "        upload += list(argmax.data)\n",
    "    with open(\"predictions.txt\", \"w\") as f:\n",
    "        f.write('Id,Cat\\n')\n",
    "        for i in range(len(upload)):\n",
    "            f.write(str(i) + \",\" + str(upload[i] + 1) + \"\\n\")\n",
    "test(cnn_model, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, you should put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
