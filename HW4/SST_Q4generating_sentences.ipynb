{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable as V\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.kl import kl_divergence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#torch text\n",
    "import torchtext.data as data\n",
    "import torchtext.datasets as datasets\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "TEXT = data.Field(lower=True, init_token=BOS_WORD, eos_token=EOS_WORD, fix_length=30)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train, val, test = datasets.SST.splits(TEXT, LABEL)\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=100))\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('vars(train[0])', vars(train[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Encoder / Inference Network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        #relevant sizes\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embed_size = embedding_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.emb_layer = nn.Embedding(self.embed_size, self.vocab_size)\n",
    "        self.emb_layer.weight.data = TEXT.vocab.vectors.clone()\n",
    "        self.enc_layer = nn.GRU(self.embed_size, self.hidden_size)\n",
    "        \n",
    "        self.mu_layer = nn.Linear(self.hidden_size, self.latent_dim)\n",
    "        self.logvar_layer = nn.Linear(self.hidden_size, self.latent_dim)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        emb = self.emb_layer(input_seq)\n",
    "        emb = self.dropout(emb)\n",
    "        _, hidden  = self.enc_layer(emb)        \n",
    "        mu = self.mu_layer(hidden)\n",
    "        logvar = self.logvar_layer(hidden)\n",
    "        return mu, logvar, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Word Generative Model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # important sizes\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embed_size = embedding_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.layer1 = nn.Linear(latent_dim, self.hidden_size)\n",
    "        self.decode_layer = nn.GRU(self.embed_size, self.hidden_size)\n",
    "        self.layer2 = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        \n",
    "    def forward(self, decoder_input, latent):\n",
    "        hidden = self.layer1(latent)\n",
    "\n",
    "        output, _ = self.decode_layer(decoder_input, hidden)\n",
    "        projection = self.layer2(output)\n",
    "\n",
    "        return F.log_softmax(projection, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE using reparameterization \"rsample\"\n",
    "class NormalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(NormalVAE, self).__init__()\n",
    "\n",
    "        # Parameters phi and computes variational parameters lambda\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Parameters theta, p(x | z)\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x_src):\n",
    "        # Example variational parameters lambda\n",
    "        mu, logvar, input_emb = self.encoder(x_src)\n",
    "        #print(logvar.mul(0.5).exp())\n",
    "        \n",
    "        q_normal = Normal(loc=mu, scale=logvar.mul(0.5).exp())\n",
    "        \n",
    "        # Reparameterized sample.\n",
    "        z_sample = q_normal.rsample()\n",
    "        #z_sample = mu\n",
    "        return self.decoder(input_emb, z_sample), mu, logvar       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is slow to run on CPU. But it shows the setup for a Miao (2016) type VAE over text. Here we use powerful encoder in the form of a LSTM. But use a very simple generative model that predicts a set of works (in binary represenation) as the output. The aim is that the latent variable should learn something akin to a topic about the words themseles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda: \n",
    "    cuda_device = 0 \n",
    "else: \n",
    "    cuda_device = -1\n",
    "    \n",
    "torch.cuda.set_device(0)\n",
    "print(cuda_device)\n",
    "\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = TEXT.vocab.stoi[\"<pad>\"]\n",
    "SOS_token = TEXT.vocab.stoi[\"<s>\"]\n",
    "EOS_token = TEXT.vocab.stoi[\"</s>\"]\n",
    "print(PAD_token, SOS_token, EOS_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_set, n_epochs, batch_size, criterion, optim, vae_model, enc_model, dec_model): \n",
    "    step = 0 \n",
    "    ELBO = [] \n",
    "    NLL = [] \n",
    "    KL = [] \n",
    "    ALPHA = [] \n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        total_kl = 0\n",
    "        total = 0\n",
    "        x0 = 500 \n",
    "        k = 0.10\n",
    "        for i, t in enumerate(train_set):\n",
    "            if t.label.size(0) != batch_size : continue\n",
    "            vae_model.zero_grad()\n",
    "            x = t.text\n",
    "            target = x[1:, :]\n",
    "            pad =  V(torch.Tensor(1, batch_size).fill_(PAD_token).long())\n",
    "            pad = pad.cuda() if use_cuda else pad \n",
    "            \n",
    "            target = torch.cat((target, pad), dim=0) \n",
    "            out, mu, logvar = vae_model(x)\n",
    "            _, sample = torch.topk(out, 1, dim=-1)\n",
    "\n",
    "            kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "            #Linear KL annealing \n",
    "            #alpha = min(1, step/x0)\n",
    "            #Logistic KL annealing\n",
    "            alpha = float(1/(1+np.exp(-k*(step-x0))))\n",
    "            \n",
    "            NLL_loss = criterion(out.view(-1, out.size()[-1]), target.view(-1))\n",
    "            loss = NLL_loss + alpha * kl \n",
    "            loss = loss / batch_size\n",
    "\n",
    "            KL.append(float(kl.data/batch_size))\n",
    "            NLL.append(float(NLL_loss.data/batch_size))\n",
    "            ELBO.append(float(loss.data))\n",
    "            ALPHA.append(alpha)\n",
    "            \n",
    "            total_loss += loss.data / batch_size\n",
    "            total_kl += kl.data / batch_size\n",
    "            \n",
    "            total += 1\n",
    "            step += 1 \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(enc_model.parameters(), 1.0)\n",
    "            torch.nn.utils.clip_grad_norm(dec_model.parameters(), 1.0)\n",
    "        print(epoch, total_loss[0] / total , total_kl[0] / total)\n",
    "    return KL, NLL, ELBO, ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "WORD_DIM = 100\n",
    "HIDDEN_DIM = 256 \n",
    "LATENT_DIM = 32\n",
    "num_embeddings = len(TEXT.vocab)\n",
    "\n",
    "train_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, test), batch_size=BATCH_SIZE, device=cuda_device, repeat=False)\n",
    "\n",
    "#bce = nn.BCEWithLogitsLoss(size_average=False)\n",
    "NLL = torch.nn.NLLLoss(size_average=False, ignore_index=PAD_token)\n",
    "\n",
    "encoder = Encoder(len(TEXT.vocab), WORD_DIM, HIDDEN_DIM, LATENT_DIM)\n",
    "decoder = Decoder(len(TEXT.vocab), WORD_DIM, HIDDEN_DIM, LATENT_DIM)\n",
    "\n",
    "encoder = encoder.cuda() if use_cuda else encoder\n",
    "decoder = decoder.cuda() if use_cuda else decoder \n",
    "\n",
    "vae = NormalVAE(encoder, decoder)\n",
    "vae = vae.cuda() if use_cuda else vae \n",
    "\n",
    "learning_rate = 0.01\n",
    "#optim = torch.optim.SGD(vae.parameters(), lr = learning_rate)\n",
    "optim = torch.optim.Adam(vae.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "if use_cuda: \n",
    "    p = Normal(V(torch.zeros(BATCH_SIZE, LATENT_DIM)).cuda(), V(torch.ones(BATCH_SIZE, LATENT_DIM)).cuda())\n",
    "else: \n",
    "    p = Normal(V(torch.zeros(BATCH_SIZE, LATENT_DIM)), V(torch.ones(BATCH_SIZE, LATENT_DIM)))\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "KL, NLL, ELBO, ALPHA = train_model(train_set=train_iter, n_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
    "            criterion=NLL, optim=optim, vae_model=vae, enc_model=encoder,\n",
    "            dec_model=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Training Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots() \n",
    "\n",
    "ax1.plot(range(len(KL)), KL, '-g')\n",
    "ax1.set_ylabel('KL Loss')\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "ax2.plot(range(len(ALPHA)), ALPHA, '-b')\n",
    "ax2.set_ylabel('KL Term Weight')\n",
    "#plt.plot(range(len(ELBO)), ELBO, label ='ELBO loss')\n",
    "#print(NLL[-10:])\n",
    "plt.xlabel('Step')\n",
    "plt.title('KL loss with logistic annealing schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal \n",
    "\n",
    "fig, ax1 = plt.subplots() \n",
    "\n",
    "ax1.plot(range(len(KL[5:])), KL[5:])\n",
    "ax1.set_ylabel('KL Loss')\n",
    "\n",
    "filtered_ELBO = scipy.signal.savgol_filter(ELBO, 101, 2)\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "# ax2.plot(range(len(ELBO)), ELBO, 'b-')\n",
    "# ax2.set_ylabel('ELBO')\n",
    "ax2.plot(range(len(ELBO[5:])), filtered_ELBO[5:], 'g-')\n",
    "ax2.set_ylabel('ELBO')\n",
    "#plt.plot(range(len(ELBO)), ELBO, label ='ELBO loss')\n",
    "#print(NLL[-10:])\n",
    "plt.xlabel('Step')\n",
    "plt.title('KL loss and filtered ELBO with logistic annealing schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(z, enc_model, dec_model, max_len): \n",
    "    t = 0 \n",
    "    b_size = z.size(0)\n",
    "    generations = torch.Tensor(max_len, b_size).fill_(PAD_token).long()\n",
    "    running_seqs = torch.arange(0, b_size, out=torch.LongTensor()).long() \n",
    "    running_seqs = running_seqs.cuda() if use_cuda else running_seqs\n",
    "    \n",
    "    hidden = dec_model.layer1(z)\n",
    "    \n",
    "    hidden = hidden.unsqueeze(0)\n",
    "    while(t < max_len): \n",
    "        if t == 0: \n",
    "            input_seq = V(torch.Tensor(b_size).fill_(SOS_token).long())\n",
    "            input_seq = input_seq.cuda() if use_cuda else input_seq\n",
    "        \n",
    "        input_seq = input_seq.unsqueeze(0)\n",
    "        \n",
    "        #embed\n",
    "        input_emb = enc_model.emb_layer(input_seq)\n",
    "        \n",
    "        output, hidden = dec_model.decode_layer(input_emb, hidden)\n",
    "        logits = F.log_softmax(dec_model.layer2(output), dim=-1)\n",
    "        \n",
    "        _, sample = torch.topk(logits, 1, dim=-1)\n",
    "        input_seq = sample.squeeze()\n",
    "        generations[t, :] = input_seq\n",
    "        \n",
    "        t += 1 \n",
    "    return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample outputs\n",
    "NUM_SAMPLES = 10 \n",
    "#sample from p(z) standard normal \n",
    "m = Normal(torch.zeros(LATENT_DIM), torch.ones(LATENT_DIM))\n",
    "sample = m.sample((NUM_SAMPLES, 1))\n",
    "sample = V(sample.squeeze(1))\n",
    "print(sample.shape)\n",
    "sample = sample.cuda() if use_cuda else sample \n",
    "\n",
    "gen = inference(sample, encoder, decoder, 15)\n",
    "for i in range(gen.size()[1]):\n",
    "    idx = gen[:, i]\n",
    "    print(\" \".join([TEXT.vocab.itos[d] for d in idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolated\n",
    "NUM_SAMPLES = 1 \n",
    "m = Normal(torch.zeros(LATENT_DIM), torch.ones(LATENT_DIM))\n",
    "z_1 = m.sample((NUM_SAMPLES, 1))\n",
    "z_2 = m.sample((NUM_SAMPLES, 1)) \n",
    "z_interpol = torch.zeros(6, LATENT_DIM)\n",
    "for i, alpha in enumerate([0, 0.2, 0.4, 0.6, 0.8, 1.0]): \n",
    "    z_comb = alpha*z_1 + (1-alpha)*z_2\n",
    "    z_interpol[i]= z_comb\n",
    "z_interpol = np.asarray(z_interpol)\n",
    "\n",
    "sample = V(torch.Tensor(z_interpol))\n",
    "sample = sample.cuda() if use_cuda else sample \n",
    "\n",
    "gen = inference(sample, encoder, decoder, 15)\n",
    "for i in range(gen.size()[1]):\n",
    "    idx = gen[:, i]\n",
    "    print(\" \".join([TEXT.vocab.itos[d] for d in idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting variational means by class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_x = [ [] for i in range(3)]\n",
    "label_y = [ [] for i in range(3)]\n",
    "full_arr = [ [] for i in range(3)]\n",
    "for datum in list(test_iter): \n",
    "    text = datum.text\n",
    "    label = datum.label\n",
    "    label -= 1 \n",
    "    _, mu, _ = encoder(text)\n",
    "    mu = mu.squeeze(0)\n",
    "    for i in range(len(label)): \n",
    "        label_x[label[i].data.cpu().numpy()].append(mu[i].data.cpu().numpy()[0])\n",
    "        label_y[label[i].data.cpu().numpy()].append(mu[i].data.cpu().numpy()[1])\n",
    "        full_arr[label[i].data.cpu().numpy()].append(mu[i].data.cpu().numpy())\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "for i, label in enumerate(['neg', 'neutral', 'pos']): \n",
    "    ax1.scatter(label_x[i], label_y[i], s = 5, label=label)\n",
    "\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([np.mean(np.asarray(label_x[i])) for i in range(3)])\n",
    "print([np.mean(np.asarray(label_y[i])) for i in range(3)])\n",
    "z_neg = np.mean(np.asarray(full_arr[0]), axis=0)\n",
    "print(z_neg)\n",
    "z_pos = np.mean(np.asarray(full_arr[1]), axis=0)\n",
    "print(z_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_interpol = torch.zeros(6, LATENT_DIM)\n",
    "z_neg = torch.Tensor(z_neg)\n",
    "z_pos = torch.Tensor(z_pos)\n",
    "for i, alpha in enumerate([0, 0.3, 0.5, 0.7, 1.0]): \n",
    "    z_comb = alpha*z_neg + (1-alpha)*z_pos\n",
    "    z_interpol[i]= z_comb\n",
    "z_interpol = np.asarray(z_interpol)\n",
    "\n",
    "sample = V(torch.Tensor(z_interpol))\n",
    "sample = sample.cuda() if use_cuda else sample \n",
    "\n",
    "gen = inference(sample, encoder, decoder, 15)\n",
    "for i in range(gen.size()[1]):\n",
    "    idx = gen[:, i]\n",
    "    print(\" \".join([TEXT.vocab.itos[d] for d in idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = iter(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_batch = next(feed)\n",
    "print(next_batch.label[:10])\n",
    "texts = next_batch.text[:, :10]\n",
    "print(texts.shape)\n",
    "for i in range(text.size()[1]): \n",
    "    print(\" \".join([TEXT.vocab.itos[d] for d in texts[:, i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.itos[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
