{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4: Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a deep generative model of binary images (MNIST) using variational autoencoders and generative adversarial networks.\n",
    "The original VAE paper can be found [here](https://arxiv.org/abs/1312.6114) and GANs [here](https://arxiv.org/abs/1406.2661), and there are many excellent tutorials\n",
    "online, e.g. [here](https://arxiv.org/abs/1606.05908) and [here](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)\n",
    "\n",
    "**For this homework there will not be a Kaggle submission**\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a discrete deep generative model of binary digits (MNIST) using variational autoencoders\n",
    "2. Examine the learned latent space with visualizations \n",
    "3. Build a conditinous deep geneartive model using generative adversarial networks.\n",
    "4. Additionally extend the above in any way, for example by :\n",
    "    - using better encoder/decoders (e.g. CNN as the encoder, PixelCNN as the decoder. Description of PixelCNN \n",
    "    can be found [here](https://arxiv.org/abs/1601.06759))\n",
    "    - using different variational families, e.g. with [normalizing flows](https://arxiv.org/abs/1505.05770), \n",
    "    [inverse autoregressive flows](https://arxiv.org/pdf/1606.04934.pdf), \n",
    "    [hierarchical models](https://arxiv.org/pdf/1602.02282.pdf)\n",
    "    - comparing with stochastic variational inference (i.e. where your variational parameters are randomly initialized and\n",
    "    then updated with gradient ascent on the ELBO\n",
    "    - or your own extension.\n",
    "\n",
    "For your encoder/decoder, we suggest starting off with simple models (e.g. 2-layer MLP with ReLUs).\n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, as always, let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default MNIST gives grayscale values between [0,1]. Since we are modeling binary images, we have to turn these\n",
    "into binary values, i.e. $\\{0,1\\}^{784}$). A standard way to do this is to interpret the grayscale values as \n",
    "probabilities and sample Bernoulli random vectors based on these probabilities. (Note you should not do this for GANs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3435)\n",
    "train_img = torch.stack([torch.bernoulli(d[0]) for d in train_dataset])\n",
    "train_label = torch.LongTensor([d[1] for d in train_dataset])\n",
    "test_img = torch.stack([torch.bernoulli(d[0]) for d in test_dataset])\n",
    "test_label = torch.LongTensor([d[1] for d in test_dataset])\n",
    "print(train_img[0])\n",
    "print(train_img.size(), train_label.size(), test_img.size(), test_label.size())\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST does not have an official train dataset. So we will use the last 10000 training points as your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img = train_img[-10000:].clone()\n",
    "val_label = train_label[-10000:].clone()\n",
    "train_img = train_img[:10000]\n",
    "train_label = train_label[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the dataloader to split into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(train_img, train_label)\n",
    "val = torch.utils.data.TensorDataset(val_img, val_label)\n",
    "test = torch.utils.data.TensorDataset(test_img, test_label)\n",
    "BATCH_SIZE = 100\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datum in train_loader:\n",
    "    img, label = datum\n",
    "    print(img.size(), label.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great now we are ready to begin modeling. Performance-wise, you want tune your hyperparameters based on the **evidence lower bound (ELBO)**. Recall that the ELBO is given by:\n",
    "\n",
    "$$ELBO = \\mathbb{E}_{q(\\mathbf{z} ; \\lambda)} [\\log p(\\mathbf{x} \\,|\\,\\mathbf{z} ; \\theta)] - \\mathbb{KL}[q(\\mathbf{z};\\lambda) \\, \\Vert \\, p(\\mathbf{z})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational parameters are given by running the encoder over the input, i..e. $\\lambda = encoder(\\mathbf{x};\\phi)$. The generative model (i.e. decoder) is parameterized by $\\theta$. Since we are working with binarized digits, $\\log p(x \\, | \\, \\mathbf{z} ; \\theta)$ is given by:\n",
    "\n",
    "$$ \\log p(x \\, | \\, \\mathbf{z} ; \\theta) = \\sum_{i=1}^{784} \\log \\sigma(\\mathbf{h})_{i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{h}$ is the final layer of the generative model (i.e. 28*28 = 784 dimensionval vector), and $\\sigma(\\cdot)$ is the sigmoid function. \n",
    "\n",
    "For the baseline model in this assignment you will be using a spherical normal prior, i.e. $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. The variational family will also be normal, i.e. $q(\\mathbf{z} ; \\lambda) = \\mathcal{N}(\\boldsymbol{\\mu}, \\log \\boldsymbol \\sigma^2)$ (here we will work with normal families with diagonal covariance). The KL-divergence between the variational posterior $q(\\mathbf{z})$ and the prior $p(\\mathbf{z})$ has a closed-form analytic solution, which is available in the original VAE paper referenced above. (If you are using the torch distributions package they will automatically calculate it for you, however you will need to use pytorch 0.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Baseline VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as img_utils\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3435)\n",
    "train_img = torch.stack([torch.bernoulli(d[0]) for d in train_dataset])\n",
    "train_label = torch.LongTensor([d[1] for d in train_dataset])\n",
    "test_img = torch.stack([torch.bernoulli(d[0]) for d in test_dataset])\n",
    "test_label = torch.LongTensor([d[1] for d in test_dataset])\n",
    "print(train_img.size(), train_label.size(), test_img.size(), test_label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img = train_img[-10000:].clone()\n",
    "val_label = train_label[-10000:].clone()\n",
    "train_img = train_img[:10000]\n",
    "train_label = train_label[:10000]\n",
    "train = torch.utils.data.TensorDataset(train_img, train_label)\n",
    "val = torch.utils.data.TensorDataset(val_img, val_label)\n",
    "test = torch.utils.data.TensorDataset(test_img, test_label)\n",
    "BATCH_SIZE = 100\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datum in train_loader:\n",
    "    img, label = datum\n",
    "    print(img.size(), label.size())\n",
    "    plt.imshow(img[3][0], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "from torch.autograd import Variable as V\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.kl import kl_divergence\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 5\n",
    "\n",
    "# Compute the variational parameters for q\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, output_size=28*28):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(output_size, 200)\n",
    "        # mean \n",
    "        self.linear2 = nn.Linear(200, LATENT_DIM)\n",
    "        # variance\n",
    "        self.linear3 = nn.Linear(200, LATENT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.linear1(x))\n",
    "        return self.linear2(h), self.linear3(h) \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size = 28*28):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(LATENT_DIM, 200)\n",
    "        self.linear2 = nn.Linear(200, output_size)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.linear1(z))\n",
    "        return F.sigmoid(self.linear2(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(NormalVAE, self).__init__()\n",
    "\n",
    "        # Parameters phi and computes variational parameters lambda\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Parameters theta, p(x | z)\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x_src):\n",
    "        # Example variational parameters lambda\n",
    "        mu, logvar = self.encoder(x_src)\n",
    "        q_normal = Normal(loc=mu, scale=logvar.mul(0.5).exp())\n",
    "        \n",
    "        # Reparameterized sample.\n",
    "        z_sample = q_normal.rsample()\n",
    "        #z_sample = mu\n",
    "        return self.decoder(z_sample), mu, logvar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bce_loss = nn.BCELoss(size_average=False)\n",
    "\n",
    "# Problem setup.\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "vae = NormalVAE(encoder, decoder)\n",
    "\n",
    "# SGD\n",
    "learning_rate = 0.02\n",
    "optim = torch.optim.SGD(vae.parameters(), lr = learning_rate)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "ELBO = [] \n",
    "KL = [] \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Keep track of reconstruction loss and total kl\n",
    "    total_loss = 0\n",
    "    total_kl = 0\n",
    "    total = 0\n",
    "    for datum in train_loader:\n",
    "        img, _ = datum\n",
    "        img = V(img)\n",
    "        img = img.view(-1, 784)\n",
    "        # Standard setup. \n",
    "        vae.zero_grad()\n",
    "        \n",
    "        # Run VAE. \n",
    "        out, mu, logvar = vae(img)\n",
    "\n",
    "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # actual loss\n",
    "        loss = bce_loss(out, img) + kl \n",
    "        loss = loss / BATCH_SIZE\n",
    "        ELBO.append(float(loss))\n",
    "        KL.append(float(kl.data/BATCH_SIZE))\n",
    "        \n",
    "        # record keeping.\n",
    "        total_loss += bce_loss(out, img).data / BATCH_SIZE\n",
    "        \n",
    "        total_kl += kl.data / BATCH_SIZE\n",
    "        total += 1\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(epoch, total_loss[0] / total , total_kl[0] / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(KL[:200])), KL[:200], label ='KL loss')\n",
    "plt.plot(range(len(ELBO[:200])), ELBO[:200], label ='ELBO loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(ELBO[200], KL[200])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def show(img, n_row=5):\n",
    "    img = img_utils.make_grid(img, n_row)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 Sample bunch of digits latent_dim = 5 \n",
    "NUM_SAMPLES = 10 \n",
    "#sample from p(z) standard normal \n",
    "m = Normal(torch.zeros(LATENT_DIM), torch.ones(LATENT_DIM))\n",
    "sample = m.sample((NUM_SAMPLES, 1))\n",
    "sample = V(sample.squeeze(1))\n",
    "\n",
    "# sample from p(x|z) decoder\n",
    "sample = decoder(sample)\n",
    "show(sample.data.view(NUM_SAMPLES, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.2 Linear Interpolation latent_dim = 5 \n",
    "z_1 = m.sample((NUM_SAMPLES, 1))\n",
    "z_2 = m.sample((NUM_SAMPLES, 1)) \n",
    "output_arr = torch.zeros((6, NUM_SAMPLES, 1, 28, 28))\n",
    "i = 0 \n",
    "for alpha in [0, 0.2, 0.4, 0.6, 0.8, 1.0]: \n",
    "    z_comb = alpha*z_1 + (1-alpha)*z_2\n",
    "    sample = decoder(z_comb)\n",
    "    output_arr[i] = sample.data.view(NUM_SAMPLES, 1, 28, 28)\n",
    "    i += 1 \n",
    "    \n",
    "# locs, labels = plt.xticks()\n",
    "# print(locs)\n",
    "# print(labels)\n",
    "plt.yticks(np.arange(0 + 15,6*30+15,30), ('0.0', '0.2', '0.4', '0.6', '0.8', '1.0'))\n",
    "plt.xticks(())\n",
    "show(output_arr.view(6*NUM_SAMPLES, 1, 28, 28) , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 latent_dim = 2 \n",
    "label_x = [ [] for i in range(10)]\n",
    "label_y = [ [] for i in range(10)]\n",
    "for datum in list(test_loader): \n",
    "    img, label = datum\n",
    "    img = V(img)\n",
    "    img = img.view(-1, 784)\n",
    "    dim = encoder(img)\n",
    "    mu = dim[0]\n",
    "    for i in range(BATCH_SIZE): \n",
    "        label_x[label[i].data.numpy()].append(mu[i].data.numpy()[0])\n",
    "        label_y[label[i].data.numpy()].append(mu[i].data.numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "for i in range(10): \n",
    "    ax1.scatter(label_x[i], label_y[i], s = 5, label=str(i))\n",
    "\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mesh grid \n",
    "mesh_grid = np.asarray(np.meshgrid(np.linspace(-2, 2, 10), np.linspace(2, -2, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot(121)\n",
    "for i in range(10): \n",
    "    ax1.scatter(label_x[i], label_y[i], s = 5, label=str(i))\n",
    "ax1.legend(loc='upper left');\n",
    "\n",
    "ax1.add_patch(patches.Rectangle(\n",
    "        (-2, -2), 4, 4, fill=False,\n",
    "        linestyle='dashed'))\n",
    "\n",
    "GRID_SIZE = 10 \n",
    "print(mesh_grid.shape)\n",
    "z_1 = mesh_grid[0].reshape(GRID_SIZE*GRID_SIZE)\n",
    "z_2 = mesh_grid[1].reshape(GRID_SIZE*GRID_SIZE)\n",
    "output_arr = torch.zeros((100, 1, 28, 28))\n",
    "i = 0 \n",
    "for i in range(100): \n",
    "    input_z = torch.Tensor([z_1[i], z_2[i]])\n",
    "    input_z = V(input_z)\n",
    "    sample = decoder(input_z)\n",
    "    output_arr[i] = sample.data.view(1, 1, 28, 28)\n",
    "    i += 1 \n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "img = img_utils.make_grid(output_arr, 10)\n",
    "npimg = img.numpy()\n",
    "ax2.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest', extent=[-2, 2, -2, 2])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#larget image\n",
    "GRID_SIZE = 10 \n",
    "\n",
    "z_1 = mesh_grid[0].reshape(GRID_SIZE*GRID_SIZE)\n",
    "z_2 = mesh_grid[1].reshape(GRID_SIZE*GRID_SIZE)\n",
    "output_arr = torch.zeros((100, 1, 28, 28))\n",
    "i = 0 \n",
    "for i in range(100): \n",
    "    input_z = torch.Tensor([z_1[i], z_2[i]])\n",
    "    input_z = V(input_z)\n",
    "    sample = decoder(input_z)\n",
    "    output_arr[i] = sample.data.view(1, 1, 28, 28)\n",
    "    i += 1 \n",
    "    \n",
    "img = img_utils.make_grid(output_arr, 10)\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest', extent=[-2, 2, -2, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GANs you should use the same data in its continuous form. Here use the same prior, but use a multi-layer network to map to a continous 28x28 output space. Then use a multilayer discriminator to classify. \n",
    "\n",
    "For both models you may also consider trying a deconvolutional network (as in DCGAN) to produce output from the latent variable.\n",
    "\n",
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to quantitative metrics (i.e. ELBO), we are also going to ask you to do some qualitative analysis via visualizations. Please include the following in your report:\n",
    "\n",
    "1. Generate a bunch of digits from your generative model (sample $\\mathbf{z} \\sim p(\\mathbf{z})$, then $\\mathbf{x} \\sim p (\\mathbf{x} \\, | \\, \\mathbf{z} ; \\theta$))\n",
    "2. Sample two random latent vectors $\\mathbf{z}_1, \\mathbf{z}_2 \\sim p(\\mathbf{z})$, then sample from their interpolated values, i.e. $\\mathbf{z} \\sim p (\\mathbf{x} \\, | \\, \\alpha\\mathbf{z}_1 + (1-\\alpha)\\mathbf{z}_2; \\theta$) for $\\alpha = \\{0, 0.2, 0.4, 0.6, 0.8 ,1.0 \\}$.\n",
    "3. Train a VAE with 2 latent dimensions. Make a scatter plot of the variational means, $\\mu_1, \\mu_2$, where the color\n",
    "corresponds to the digit.\n",
    "4. With the same model as in (3), pick a 2d grid around the origin (0,0), e.g. with\n",
    "`np.meshgrid(np.linspace(-2, 2, 10), np.linspace(-2, 2, 10)`. For each point in the grid $(z_1, z_2)$, generate\n",
    "$\\mathbf{x}$ and show the corresponding digit in the 2d plot. For an example see [here](http://fastforwardlabs.github.io/blog-images/miriam/tableau.1493x693.png) (the right image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
