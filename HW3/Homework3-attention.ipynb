{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to process the raw data using a tokenizer. We are going to be using spacy, which can be installed via:  \n",
    "  `[sudo] pip install spacy`  \n",
    "  \n",
    "Tokenizers for English/German can be installed via:  \n",
    "  `[sudo] python -m spacy download en`  \n",
    "  `[sudo] python -m spacy download de`\n",
    "  \n",
    "This isn't *strictly* necessary, and you can use your own tokenization rules if you prefer (e.g. a simple `split()` in addition to some rules to acccount for punctuation), but we recommend sticking to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to add the beginning-of-sentence token `<s>` and the end-of-sentence token `</s>` to the \n",
    "target so we know when to begin/end translating. We do not need to do this on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data. This may take a few minutes.\n",
    "\n",
    "**While this dataset of 200K sentence pairs is relatively small compared to others, it will still take some time to train. So we are going to be only working with sentences of length at most 20 for this homework. Please train only on this reduced dataset for this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary and convert the text corpus into indices. We are going to be replacing tokens that occurred less than 5 times with `<unk>` tokens, and take the rest as our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our data into batches as usual. Batching for MT is slightly tricky because source/target will be of different lengths. Fortunately, `torchtext` lets you do this by allowing you to pass in a `sort_key` function. This will minimizing the amount of padding on the source side, but since there is still some padding you will inadvertendly \"attend\" to these padding tokens. \n",
    "\n",
    "One way to get rid of padding is to pass a binary `mask` vector to your attention module so its attention score (before the softmax) is minus infinity for the padding token. Another way (which is how we do it for our projects, e.g. opennmt) is to manually sort data into batches so that each batch has exactly the same source length (this means that some batches will be less than the desired batch size, though).\n",
    "\n",
    "However, for this homework padding won't matter too much, so it's fine to ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=0,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see that the BOS/EOS token is indeed appended to the target (English) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "print(\"Target\")\n",
    "print(batch.trg)\n",
    "print(batch.src.volatile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now that we've processed the data, we are ready to begin modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': <torchtext.data.field.Field object at 0x7fa53688c278>, 'trg': <torchtext.data.field.Field object at 0x7fa53688c4a8>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n",
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13353\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Set up \n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS\n",
    "\n",
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))\n",
    "\n",
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "# print(DE.vocab.stoi[\"<s>\"], DE.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>\n",
    "print(EN.vocab.stoi[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(DE.vocab.stoi[\"<pad>\"])\n",
    "BATCH_SIZE = 64\n",
    "if use_cuda: \n",
    "    train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=0,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))\n",
    "else: \n",
    "    train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "#     print(\"Target\")\n",
    "#print(batch.trg)\n",
    "print(len(list(train_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "PAD_token = 1 \n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, h_size, batch_size, n_layers=1, dropout=0, bidir=False):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.num_layers = n_layers\n",
    "        self.hidden_size = h_size\n",
    "        self.batch_size = batch_size\n",
    "        self.bidir=bidir\n",
    "        self.embed = nn.Embedding(input_size, h_size)\n",
    "        self.lstm = nn.LSTM(h_size, h_size, dropout=dropout, num_layers=n_layers, bidirectional=bidir)\n",
    "\n",
    "    def forward(self, input_src, hidden):\n",
    "        embedded = self.embed(input_src)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        result = (Variable(torch.zeros(self.num_layers + int(self.bidir) , self.batch_size, self.hidden_size)),\n",
    "                  Variable(torch.zeros(self.num_layers + int(self.bidir) , self.batch_size, self.hidden_size)))\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers + int(self.bidir) , self.batch_size, self.hidden_size)).cuda(),\n",
    "                    Variable(torch.zeros(self.num_layers + int(self.bidir) , self.batch_size, self.hidden_size)).cuda())\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "        if use_cuda:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "            \n",
    "       # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_output):  \n",
    "        energy = hidden.dot(encoder_output)\n",
    "        return energy\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def train_batch(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LEN):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    encoder.train() \n",
    "    decoder.train()\n",
    "    \n",
    "    #useful lengths \n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    batch_size = target_variable.size()[1]\n",
    "    \n",
    "    # zero words and zero loss \n",
    "    loss = 0 \n",
    "    total_words = 0 \n",
    "    \n",
    "    encoder_output_short, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "    \n",
    "    #pad encoder outputs for attention \n",
    "    encoder_outputs[:input_length, :, :] = encoder_output_short \n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    #initialize last row as padding tokens\n",
    "    last_row = torch.ones(1, batch_size).long()\n",
    "    last_row = last_row.cuda() if use_cuda else last_row\n",
    "    \n",
    "    shifted_target = Variable(torch.cat((target_variable[1:, :].data.long(), last_row)))\n",
    "    output_words = Variable(torch.ones(target_length, batch_size))\n",
    "    output_words = output_words.cuda() if use_cuda else output_words\n",
    "    \n",
    "    for t in range(target_length):\n",
    "        decoder_input = target_variable[t].view(1, -1)\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        \n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "#         debug attention\n",
    "#         v, i = torch.max(decoder_attention.squeeze(1), 1)\n",
    "#         print(i)\n",
    "        \n",
    "        v, i = torch.max(decoder_output.squeeze(1), 1)\n",
    "        output_words[t] = i\n",
    "        \n",
    "        loss += criterion(decoder_output.squeeze(0), shifted_target[t])\n",
    "        total_words += shifted_target[t].ne(PAD_token).int().sum()\n",
    "\n",
    "#     debug output \n",
    "#     print(output_words)\n",
    "#     print(shifted_target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), 3.0)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 3.0)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    print(loss.data[0])\n",
    "    print(total_words.data[0])\n",
    "    return loss.data[0]/total_words.data[0]\n",
    "\n",
    "\n",
    "def validate(encoder, decoder, val_iter, criterion, max_length = MAX_LEN):\n",
    "    encoder.eval() \n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    total_words = 0 \n",
    "    for batch in iter(val_iter):\n",
    "        \n",
    "        input_length = batch.src.size()[0]\n",
    "        target_length = batch.trg.size()[0]\n",
    "        batch_size = batch.src.size()[1]\n",
    "        if batch_size != 64:\n",
    "            break\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "        \n",
    "        encoder_output_short, encoder_hidden = encoder(batch.src, encoder_hidden)\n",
    "    \n",
    "        encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "        encoder_outputs[:input_length, :, :] = encoder_output_short      \n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "            #initialize last row as padding tokens\n",
    "        last_row = torch.ones(1, batch_size).long()\n",
    "        last_row = last_row.cuda() if use_cuda else last_row\n",
    "\n",
    "        shifted_target = Variable(torch.cat((batch.trg[1:, :].data.long(), last_row)))\n",
    "        output_words = Variable(torch.ones(target_length, batch_size))\n",
    "        output_words = output_words.cuda() if use_cuda else output_words\n",
    "        loss = 0 \n",
    "        for t in range(target_length):\n",
    "            decoder_input = batch.trg[t].view(1, -1)\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "            loss += criterion(decoder_output.squeeze(0), shifted_target[t])\n",
    "            total_words += shifted_target[t].ne(PAD_token).int().sum()\n",
    "\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "    return total_loss / total_words.data[0]\n",
    "\n",
    "\n",
    "def trainIters(encoder, decoder, training_iter, valid_iter, target_vocab_len, learning_rate=0.7, num_epochs=20):\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    #mask weight to not consider in loss \n",
    "    mask_weight = Variable(torch.FloatTensor(target_vocab_len).fill_(1))\n",
    "    mask_weight[PAD_token] = 0\n",
    "    mask_weight = mask_weight.cuda() if use_cuda else mask_weight\n",
    "    \n",
    "    #pass mask weight in to NLL Loss without size_average\n",
    "    criterion = nn.NLLLoss(weight=mask_weight, size_average=False)\n",
    "    val_loss = validate(encoder, decoder, valid_iter, criterion)\n",
    "    print(\"val loss: \", val_loss)\n",
    "    print(\"val ppl: \", np.exp(val_loss))\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        #initialise total loss and batch count\n",
    "        batch_len = 0\n",
    "        total_loss = 0\n",
    "        for batch in iter(training_iter):\n",
    "            if batch.src.size()[1] == 64: \n",
    "                loss = train_batch(batch.src, batch.trg, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "                print(loss)\n",
    "                total_loss += loss\n",
    "                batch_len += 1\n",
    "            \n",
    "        # divide total loss by batch_length\n",
    "        train_loss = total_loss / batch_len\n",
    "        \n",
    "        print(\"train loss: \", train_loss)\n",
    "        print(\"train ppl: \", np.exp(train_loss))\n",
    "        val_loss = validate(encoder, decoder, valid_iter, criterion)\n",
    "        print(\"val loss: \", val_loss)\n",
    "        print(\"val ppl: \", np.exp(val_loss))\n",
    "        \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size, dropout=0.1, n_layers=1, max_length=MAX_LEN):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = Attn(hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.lstm = nn.LSTM(self.hidden_size*2, self.hidden_size, num_layers=n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "\n",
    "    def forward(self, input_data, hidden, encoder_outputs):\n",
    "        #input_len x batch_size \n",
    "        embedded = self.embedding(input_data) #1 x batch_size x hidden dim\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        #embedded[0] is 1 x batch_size x hidden_dim  \n",
    "        #hidden[0] hn is 1 x batch_size x hidden_dim \n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "\n",
    "        attn_weights = self.attn(hidden[0], encoder_outputs)\n",
    "\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "\n",
    "        context = context.transpose(0, 1)\n",
    "\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((embedded, context), 2)\n",
    "        output, hidden = self.lstm(rnn_input, hidden)\n",
    "        # Final output layer\n",
    "        output = output.squeeze(0)\n",
    "        output = self.out(torch.cat((output, context.squeeze(0)), 1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        result = (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                  Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)))\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)).cuda(),\n",
    "                    Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)).cuda())\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 200\n",
    "encoder1 = EncoderLSTM(len(DE.vocab), hidden_size, batch_size=64, dropout=0.3, n_layers=1)\n",
    "decoder1 = AttnDecoderRNN(hidden_size, len(EN.vocab), batch_size=64, dropout=0.3, n_layers=1)\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    decoder1 = decoder1.cuda()\n",
    "trainIters(encoder1, decoder1, list(train_iter)[:200], val_iter, len(EN.vocab), num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.FloatTensor(len(EN.vocab)).fill_(1)\n",
    "weight[1] = 0\n",
    "weight = weight.cuda() if use_cuda else weight\n",
    "criterion = nn.NLLLoss(weight=Variable(weight), size_average=False)\n",
    "val_loss = validate(encoder1, decoder1, val_iter, criterion)\n",
    "# print(\"val loss: \", val_loss)\n",
    "# print(\"val ppl: \", np.exp(val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, batch, max_length=MAX_LEN):\n",
    "\n",
    "    target_length = batch.trg.size()[0]\n",
    "    batch_length = batch.src.size()[1]\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    _, encoder_hidden = encoder(batch.src, encoder_hidden)\n",
    "    decoder_input = Variable(torch.ones(1, batch_length).long()*SOS_token)\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "\n",
    "    decoded_words = []\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = torch.max(decoder_output, 2)\n",
    "        decoded_words.append(topi)\n",
    "        decoder_input = topi\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            \n",
    "    return torch.cat(decoded_words)\n",
    "\n",
    "batch = list(iter(val_iter))[8]\n",
    "# print(\"Source\")\n",
    "# print(batch.src)\n",
    "# print(\"Target\")\n",
    "# print(batch.trg)\n",
    "evaluate(encoder1, decoder1, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidir models\n",
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "PAD_token = 1 \n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, h_size, batch_size, n_layers=1, dropout=0, bidir=False):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.num_layers = n_layers\n",
    "        self.hidden_size = h_size\n",
    "        self.batch_size = batch_size\n",
    "        self.bidir=bidir\n",
    "        self.embed = nn.Embedding(input_size, h_size)\n",
    "        self.lstm = nn.LSTM(h_size, h_size, dropout=dropout, num_layers=n_layers, bidirectional=bidir)\n",
    "\n",
    "    def forward(self, input_src, hidden):\n",
    "        embedded = self.embed(input_src)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.bidir: \n",
    "            bi_dir_layers  = 2\n",
    "        else: \n",
    "            bi_dir_layers  = 1\n",
    "        result = (Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)),\n",
    "                  Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)))\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)).cuda(),\n",
    "                    Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)).cuda())\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        # hidden -> target_len x batch_size x hidden_dim\n",
    "        hidden = hidden.transpose(0, 1) # batch_size x target_len x hidden_dim\n",
    "        \n",
    "        # encoder_outputs -> max_len x batch_size x hidden_dim\n",
    "        encoder_outputs = encoder_outputs.permute(1, 2, 0)\n",
    "        \n",
    "        attn_energies = torch.bmm(hidden, encoder_outputs) # B x S\n",
    "        \n",
    "\n",
    "        return F.softmax(attn_energies, dim=2)\n",
    "        \n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size, n_layers=1, dropout=0):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, dropout=dropout, num_layers=n_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(self.dropout(output))\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size, dropout=0.1, n_layers=1, max_length=MAX_LEN):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size*2\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = Attn(hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(self.hidden_size*2, self.hidden_size, num_layers=n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "\n",
    "    def forward(self, input_data, hidden, encoder_outputs):\n",
    "        #input_len x batch_size \n",
    "\n",
    "        embedded = self.embedding(input_data) #batch_size x target_len x hidden dim\n",
    "        embedded = F.relu(embedded)\n",
    "        #lstm_output -> target_len x batch_size x hidden_dim\n",
    "        lstm_output, lstm_hidden = self.lstm(embedded, hidden)\n",
    "\n",
    "        #attn input 0 to T-1 \n",
    "        if hidden[0].size()[0] != 1: \n",
    "            attn_hidden = hidden[0][-1].unsqueeze(0)\n",
    "        else: \n",
    "            attn_hidden = hidden[0]\n",
    "        \n",
    "        if(lstm_output.size()[0] > 1):  \n",
    "            attn_input = torch.cat((attn_hidden, lstm_output[:-1]))\n",
    "        else: \n",
    "            attn_input = attn_hidden\n",
    "        # encoder_outputs -> max_len x batch_size x hidden_dim\n",
    "        attn_weights = self.attn(attn_input, encoder_outputs)\n",
    "        \n",
    "        # context = batch_size x target_length x hidden_dim \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) \n",
    "        \n",
    "        context = context.transpose(1, 0) #target_length x batch_size x hidden_dim\n",
    "        \n",
    "        output = torch.cat((lstm_output, context), 2)\n",
    "\n",
    "        # Final output layer\n",
    "        final_output = F.log_softmax(self.out(output), dim=2)\n",
    "        final_output = self.dropout(final_output)\n",
    "        return final_output, lstm_hidden, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        result = (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                  Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)))\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)).cuda(),\n",
    "                    Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)).cuda())\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "def train_batch(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LEN):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    #useful lengths \n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    batch_size = target_variable.size()[1]\n",
    "    layers = encoder.num_layers\n",
    "    # zero words and zero loss \n",
    "    loss = 0 \n",
    "    total_words = 0 \n",
    "    encoder_output_short, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    encoder_outputs = Variable(torch.zeros(max_length, batch_size, 2*encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "    \n",
    "    #ATTENTION\n",
    "    encoder_outputs[:input_length, :, :] = encoder_output_short \n",
    "\n",
    "    if layers != 1: \n",
    "        decoder_hidden = (torch.cat((encoder_hidden[0][-layers:], encoder_hidden[0][-layers:]), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][-layers:], encoder_hidden[1][-layers:]), dim=2)) \n",
    "    else: \n",
    "         decoder_hidden = (torch.cat((encoder_hidden[0][0].unsqueeze(0), encoder_hidden[0][1].unsqueeze(0)), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][0].unsqueeze(0), encoder_hidden[1][1].unsqueeze(0)), dim=2)) \n",
    "        \n",
    "    #using encoder output and target variable \n",
    "\n",
    "    decoder_output, decoder_hidden, decoder_attention = decoder(target_variable, decoder_hidden, encoder_outputs)\n",
    "    \n",
    "    #initialize last row as padding tokens\n",
    "    last_row = torch.ones(1, batch_size).long()\n",
    "    last_row = last_row.cuda() if use_cuda else last_row\n",
    "    \n",
    "    #shift target from 1:n + padding row\n",
    "    shifted_target = Variable(torch.cat((target_variable[1:, :].data.long(), last_row)))\n",
    "    m, i = torch.max(decoder_output, dim=2)\n",
    "\n",
    "    #calculate decoder_output loss with shifted target loss\n",
    "\n",
    "    loss = criterion(decoder_output.view(target_length*batch_size, -1), shifted_target.view(target_length*batch_size))\n",
    "    # count total words\n",
    "    total_words = shifted_target.ne(PAD_token).int().sum()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), 3.0)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 3.0)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.data[0]/total_words.data[0]\n",
    "\n",
    "\n",
    "def validate(encoder, decoder, val_iter, criterion, max_length = MAX_LEN):\n",
    "    encoder.eval() \n",
    "    decoder.eval() \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    total_words = 0 \n",
    "    for batch in iter(val_iter):\n",
    "        num_batches += 1 \n",
    "        input_length = batch.src.size()[0]\n",
    "        target_length = batch.trg.size()[0]\n",
    "        batch_size = batch.src.size()[1]\n",
    "        layers = encoder.num_layers\n",
    "        if batch_size != 64:\n",
    "            break\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "         \n",
    "        encoder_output_short, encoder_hidden = encoder(batch.src, encoder_hidden)\n",
    "        encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size*2))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "        encoder_outputs[:input_length, :, :] = encoder_output_short      \n",
    "\n",
    "        if layers != 1: \n",
    "            decoder_hidden = (torch.cat((encoder_hidden[0][-layers:], encoder_hidden[0][-layers:]), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][-layers:], encoder_hidden[1][-layers:]), dim=2)) \n",
    "        else: \n",
    "            decoder_hidden = (torch.cat((encoder_hidden[0][0].unsqueeze(0), encoder_hidden[0][1].unsqueeze(0)), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][0].unsqueeze(0), encoder_hidden[1][1].unsqueeze(0)), dim=2)) \n",
    "        \n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(batch.trg, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        m, i = torch.max(decoder_output, dim=2)\n",
    "        plot_attention(decoder_attention[0, :, :].data.cpu().numpy(), batch.src.data[:, 0].cpu().numpy(), i.data[:, 0].cpu().numpy())\n",
    "                    \n",
    "        first_row = torch.ones(1, batch_size).long()\n",
    "        first_row = first_row.cuda() if use_cuda else first_row\n",
    "        \n",
    "        shifted_target = Variable(torch.cat((batch.trg[1:, :].data.long(), first_row)))\n",
    "        loss = criterion(decoder_output.view(target_length*batch_size, -1), shifted_target.view(target_length*batch_size))\n",
    "        total_words += shifted_target.ne(PAD_token).int().sum()\n",
    "\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "    return total_loss / total_words.data[0]\n",
    "\n",
    "def plot_attention(d_attn, source_seq, target_seq):  \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(d_attn, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    source_sentence = [DE.vocab.itos[j] for j in source_seq]\n",
    "    target_sentence = [EN.vocab.itos[j] for j in target_seq]\n",
    "    ax.set_xticklabels([''] + source_sentence, rotation=90)\n",
    "    ax.set_yticklabels([''] + target_sentence)\n",
    "\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def trainIters(encoder, decoder, training_iter, valid_iter, target_vocab_len, learning_rate=0.7, num_epochs=20):\n",
    "    encoder.train() \n",
    "    decoder.train() \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    #mask weight to not consider in loss \n",
    "    mask_weight = Variable(torch.FloatTensor(target_vocab_len).fill_(1))\n",
    "    mask_weight[PAD_token] = 0\n",
    "    mask_weight = mask_weight.cuda() if use_cuda else mask_weight\n",
    "    \n",
    "    #pass mask weight in to NLL Loss without size_average\n",
    "    criterion = nn.NLLLoss(weight=mask_weight, size_average=False)\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        #initialise total loss and batch count\n",
    "        batch_len = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in iter(training_iter):\n",
    "            if batch.src.size()[1] == 64: \n",
    "                loss = train_batch(batch.src, batch.trg, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "                total_loss += loss\n",
    "                batch_len += 1\n",
    "            \n",
    "        # divide total loss by batch_length\n",
    "        train_loss = total_loss / batch_len\n",
    "        \n",
    "        print(\"train loss: \", train_loss)\n",
    "        print(\"train ppl: \", np.exp(train_loss))\n",
    "        val_loss = validate(encoder, decoder, valid_iter, criterion)\n",
    "        print(\"val loss: \", val_loss)\n",
    "        print(\"val ppl: \", np.exp(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 200\n",
    "encoder1 = EncoderLSTM(len(DE.vocab), hidden_size, batch_size=64, dropout=0.3, n_layers=2, bidir=True)\n",
    "decoder1 = AttnDecoderRNN(hidden_size, len(EN.vocab), batch_size=64, dropout=0.3, n_layers=2)\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    decoder1 = decoder1.cuda()\n",
    "trainIters(encoder1, decoder1, train_iter, val_iter, len(EN.vocab), num_epochs=5)\n",
    "# mask_weight = Variable(torch.FloatTensor(len(EN.vocab)).fill_(1))\n",
    "# mask_weight[PAD_token] = 0\n",
    "# mask_weight = mask_weight.cuda() if use_cuda else mask_weight\n",
    "# criterion = nn.NLLLoss(weight=mask_weight, size_average=False)\n",
    "# val_loss = validate(encoder1, decoder1, val_iter, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(beams, k):\n",
    "    \"\"\"\n",
    "    Prunes all but the top k beams, by summative score\n",
    "    \"\"\"\n",
    "    beams.sort(key=lambda x: x[1], reverse=True) #sort beams by second element (score)\n",
    "    return beams[:k] #return top k\n",
    "\n",
    "def evaluate_kaggle(encoder, decoder, string, k = 3, ngrams = 3, max_length = 20, batch_size=1):\n",
    "    # Run string through encoder\n",
    "\n",
    "    encoder_input = string.unsqueeze(1).expand(-1, batch_size)\n",
    "\n",
    "    layers = encoder.num_layers\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_output_short, encoder_hidden = encoder(encoder_input, encoder_hidden)\n",
    "    \n",
    "    #expand encoder outputs\n",
    "    input_length = string.size()[0]\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size*2))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    " \n",
    "    print(encoder_output_short.shape)\n",
    "    print(encoder_outputs.shape)\n",
    "    encoder_outputs[:input_length, :, :] = encoder_output_short\n",
    "    \n",
    "    #decoder_input = Variable(torch.ones(1, batch_length).long()*SOS_token)\n",
    "\n",
    "    decoder_input = Variable(torch.ones(1, batch_size).long()*SOS_token) #1 x batch_length\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    if layers != 1: \n",
    "        decoder_hidden = (torch.cat((encoder_hidden[0][-layers:], encoder_hidden[0][-layers:]), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][-layers:], encoder_hidden[1][-layers:]), dim=2)) \n",
    "    else: \n",
    "         decoder_hidden = (torch.cat((encoder_hidden[0][0].unsqueeze(0), encoder_hidden[0][1].unsqueeze(0)), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][0].unsqueeze(0), encoder_hidden[1][1].unsqueeze(0)), dim=2)) \n",
    "      \n",
    "\n",
    "    # base case - get top k predictions from SOS_token\n",
    "    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "    # Get most likely word index from output\n",
    "    \n",
    "    topk_probs, topk_word_idx = decoder_output.data.topk(k, dim = 2)\n",
    "    print(topk_word_idx[:, 0].shape)\n",
    "    print(' '.join([EN.vocab.itos[id] for id in topk_word_idx[:, 0][0]]))\n",
    "    decoder_input = Variable(topk_word_idx[:, 0]) # Chosen word is next input\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    beam_outputs = [([topk_word_idx.view(-1)[i]], topk_probs.view(-1)[i]) for i in range(k)]\n",
    "    \n",
    "\n",
    "    # non base case\n",
    "    for trg_word_idx in range(0, ngrams - 1): # <s> shouldn't count\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "\n",
    "        # Get most likely word index from output\n",
    "        topk_probs, topk_word_idx = decoder_output.data.topk(k, dim = 2)\n",
    "        \n",
    "        #create top k*k-by-1 matrix\n",
    "        temp_beam = []\n",
    "        for i in range(k):\n",
    "            beam_words = beam_outputs[i][0]\n",
    "            beam_score = beam_outputs[i][1]\n",
    "            for j in range(k):\n",
    "                index = i * k + j\n",
    "                curr_word_index = topk_word_idx.view(-1)[index]\n",
    "                curr_score = topk_probs.view(-1)[index]\n",
    "                \n",
    "                temp_beam.append((beam_words + [curr_word_index], beam_score + curr_score))\n",
    "                \n",
    "        \n",
    "        #prune k*k-by-1 matrix to 1xk\n",
    "        beam_outputs = prune(temp_beam, k)\n",
    "        #print(beam_outputs)\n",
    "        #set beams equal to decoder_input\n",
    "        new_beams = [beam[0] for beam in beam_outputs]\n",
    "\n",
    "        new_beam_input = [[beam[0][-1]] for beam in beam_outputs]\n",
    "        #new_beam_input = new_beams\n",
    "        decoder_input = Variable(torch.LongTensor(new_beam_input)).transpose(0,1) # Chosen beams are next input\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        \n",
    "\n",
    "    #out_idx = zip(*beam_outputs)\n",
    "    kaggle_outputs = ['|'.join([EN.vocab.itos[id] for id in beam]) for beam in new_beams]\n",
    "    return ' '.join(kaggle_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "encoder2 = EncoderLSTM(len(DE.vocab), hidden_size, batch_size=100, dropout=0.3, n_layers=2, bidir=True)\n",
    "decoder2 = AttnDecoderRNN(hidden_size, len(EN.vocab), batch_size=100, dropout=0.3, n_layers=2)\n",
    "\n",
    "encoder2.load_state_dict(torch.load('10_attn_encoder_model.pt'))\n",
    "decoder2.load_state_dict(torch.load('10_attn_decoder_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "best ... getting information long place happened Okay real after bit own brain done might system left 10 between am believe pretty simple took goes number picture looks person kids able anything second women feel found first little look than other when much could who into also because story every percent year called many doing her any big today Because thought idea Why No way their or said some now time There here these know -- would your What want So \" this ? <pad> <unk> to and But can do what one 're This me only where back which\n",
      "I|want|to\n",
      "['best|answer|means', 'best|answer|happens', 'best|answer|10', 'best|answer|help', 'best|answer|off', 'best|answer|My', 'best|answer|fact', 'best|answer|better', 'best|answer|again', 'best|answer|idea', 'best|answer|went', 'best|answer|another', 'best|answer|Why', 'best|answer|thought', 'best|answer|No', 'best|answer|bit', 'best|answer|live', 'best|answer|second', 'best|answer|feel', 'best|answer|found', 'best|answer|four', 'best|answer|looking', 'best|answer|end', 'best|answer|Okay', 'best|answer|took', 'best|answer|simple', 'best|answer|children', 'best|answer|school', 'best|answer|comes', 'best|answer|am', 'best|answer|money', 'best|answer|OK', 'best|answer|ago', 'best|answer|old', 'best|answer|real', 'best|answer|brain', 'best|answer|also', 'best|answer|need', 'best|answer|Well', 'best|answer|those', 'best|answer|their', 'best|answer|then', 'best|answer|some', 'best|answer|us', 'best|answer|when', 'best|answer|much', 'best|answer|make', 'best|answer|into', 'best|answer|day', 'best|answer|should', 'best|answer|percent', 'best|answer|A', 'best|answer|use', 'best|answer|down', 'best|answer|If', 'best|answer|course', 'best|answer|mean', 'best|answer|!', 'best|answer|tell', 'best|answer|problem', 'best|answer|big', 'best|answer|Because', 'best|answer|ca', 'best|answer|These', 'best|answer|now', 'best|answer|What', \"best|answer|'ve\", 'best|answer|has', 'best|answer|were', 'best|answer|see', 'best|answer|more', 'best|answer|had', 'best|answer|as', 'best|answer|just', 'best|answer|like', 'best|answer|very', 'best|answer|our', 'best|answer|from', 'best|answer|here', 'best|answer|them', 'best|answer|do', 'best|answer|So', 'best|answer|?', 'best|answer|\"', 'best|answer|,', 'best|answer|.', 'best|answer|is', 'best|answer|in', 'best|answer|with', 'best|answer|not', 'best|answer|about', 'best|answer|can', 'best|answer|The', 'best|answer|But', 'best|answer|there', 'best|answer|all', 'best|answer|How', 'best|answer|kind', 'best|answer|which', 'best|answer|lot']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "home thinking man Then wanted million happened whole brain real own better system might old All Africa understand money 10 interesting school five comes looks hard help play best bad light getting live sort found pretty around new great well lot 'll than back life different even down any went mean These problem story being today example human idea Why My love off fact look first also little been other who need would had 'm Now now want when two with not was are is </s> 's that as 're be The here know them really should year ; A\n",
      "Who|sends|whom\n",
      "['home|enough|People', 'home|enough|maybe', 'home|enough|Or', 'home|enough|inside', 'home|enough|children', 'home|enough|pretty', 'home|enough|interesting', 'home|enough|five', 'home|enough|happened', 'home|enough|long', 'home|enough|All', 'home|enough|When', 'home|enough|ever', 'home|enough|four', 'home|enough|Okay', 'home|enough|women', 'home|enough|future', 'home|enough|light', 'home|enough|best', 'home|enough|become', 'home|enough|goes', 'home|enough|Is', 'home|enough|kids', 'home|enough|power', 'home|enough|building', 'home|enough|moment', 'home|enough|makes', 'home|enough|As', 'home|enough|making', 'home|enough|guy', 'home|enough|space', 'home|enough|half', 'home|enough|looks', 'home|enough|For', 'home|enough|try', 'home|enough|understand', 'home|enough|course', 'home|enough|made', 'home|enough|A', 'home|enough|through', 'home|enough|even', \"home|enough|'ll\", 'home|enough|first', 'home|enough|look', 'home|enough|let', 'home|enough|take', 'home|enough|great', 'home|enough|;', 'home|enough|its', 'home|enough|next', 'home|enough|him', 'home|enough|bit', 'home|enough|again', 'home|enough|any', 'home|enough|question', 'home|enough|still', 'home|enough|brain', 'home|enough|done', 'home|enough|water', 'home|enough|after', 'home|enough|might', 'home|enough|came', 'home|enough|old', 'home|enough|before', 'home|enough|also', 'home|enough|does', 'home|enough|into', 'home|enough|who', 'home|enough|will', 'home|enough|much', 'home|enough|got', 'home|enough|years', 'home|enough|he', 'home|enough|more', 'home|enough|out', 'home|enough|really', 'home|enough|then', 'home|enough|want', 'home|enough|or', 'home|enough|said', 'home|enough|:', 'home|enough|about', 'home|enough|was', \"home|enough|n't\", 'home|enough|a', 'home|enough|,', \"home|enough|'s\", 'home|enough|in', 'home|enough|like', 'home|enough|-', \"home|enough|'re\", 'home|enough|there', 'home|enough|from', 'home|enough|an', 'home|enough|--', 'home|enough|up', 'home|enough|went', 'home|enough|ca', \"home|enough|'d\", 'home|enough|Because']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "best else thinking makes end quite Okay women When always brain before million ago together looking For Africa 10 understand am took happens stuff Is number play looks goes answer high may believe everything school five ; doing made year work good something Well different lot life three start important any another Because started big Here bit him still No own fact One its make years Thank when way their much world would get see how but What time then all there they But the <unk> 's this as just This at me an know out being through every problem\n",
      "This|person|has\n",
      "['best|As|billion', 'best|As|half', 'best|As|mind', 'best|As|seen', 'best|As|wanted', 'best|As|ago', 'best|As|looking', 'best|As|found', 'best|As|fact', 'best|As|My', 'best|As|She', 'best|As|love', 'best|As|next', 'best|As|find', 'best|As|came', 'best|As|might', 'best|As|country', 'best|As|...', 'best|As|anything', 'best|As|power', 'best|As|help', 'best|As|10', 'best|As|looks', 'best|As|goes', 'best|As|wrong', 'best|As|small', 'best|As|makes', 'best|As|talking', 'best|As|times', 'best|As|guy', 'best|As|hand', 'best|As|energy', 'best|As|simple', 'best|As|everything', 'best|As|pretty', 'best|As|away', 'best|As|say', 'best|As|where', 'best|As|over', 'best|As|only', 'best|As|does', 'best|As|who', 'best|As|He', 'best|As|years', 'best|As|good', 'best|As|first', 'best|As|those', 'best|As|which', 'best|As|today', 'best|As|story', 'best|As|percent', 'best|As|day', 'best|As|come', 'best|As|new', 'best|As|use', 'best|As|course', 'best|As|another', 'best|As|change', 'best|As|mean', 'best|As|big', 'best|As|important', 'best|As|idea', 'best|As|give', 'best|As|still', 'best|As|go', 'best|As|then', 'best|As|now', 'best|As|some', 'best|As|did', \"best|As|'m\", 'best|As|get', 'best|As|want', 'best|As|You', 'best|As|like', 'best|As|one', 'best|As|at', 'best|As|these', 'best|As|They', 'best|As|them', 'best|As|out', 'best|As|and', 'best|As|in', 'best|As|you', 'best|As|it', 'best|As|<s>', 'best|As|<pad>', 'best|As|the', \"best|As|'s\", 'best|As|We', \"best|As|n't\", 'best|As|this', 'best|As|?', \"best|As|'re\", 'best|As|be', 'best|As|there', 'best|As|all', 'best|As|doing', 'best|As|down', 'best|As|his', 'best|As|take']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "without nothing energy yet quite point women pretty All before real few When used ago together thinking future country getting goes understand anything may social As makes create times moment making sense stuff means between money first Well those say Thank will no way been years thing also today ! story being use doing course percent start thought put important love She fact water go said or things has how What time just at one This were know more had this it And you <pad> <unk> . is We n't was So : with The 're three kind only life\n",
      "The|technologies|do\n",
      "['without|ask|mind', 'without|ask|maybe', 'without|ask|People', 'without|ask|told', 'without|ask|pretty', 'without|ask|women', 'without|ask|interesting', 'without|ask|took', 'without|ask|done', 'without|ask|last', 'without|ask|part', 'without|ask|better', 'without|ask|technology', 'without|ask|used', 'without|ask|each', 'without|ask|feel', 'without|ask|makes', 'without|ask|else', 'without|ask|anything', 'without|ask|dollars', 'without|ask|goes', 'without|ask|answer', 'without|ask|kids', 'without|ask|able', 'without|ask|says', 'without|ask|true', 'without|ask|using', 'without|ask|hand', 'without|ask|works', 'without|ask|less', 'without|ask|half', 'without|ask|billion', 'without|ask|number', 'without|ask|help', 'without|ask|between', 'without|ask|10', 'without|ask|where', 'without|ask|work', 'without|ask|lot', 'without|ask|say', 'without|ask|also', 'without|ask|something', 'without|ask|years', 'without|ask|into', 'without|ask|than', 'without|ask|look', 'without|ask|good', 'without|ask|those', 'without|ask|talk', 'without|ask|problem', 'without|ask|day', 'without|ask|tell', 'without|ask|course', 'without|ask|;', 'without|ask|year', 'without|ask|should', 'without|ask|idea', 'without|ask|Here', 'without|ask|being', 'without|ask|Because', 'without|ask|start', 'without|ask|question', 'without|ask|him', 'without|ask|fact', 'without|ask|He', 'without|ask|right', 'without|ask|now', 'without|ask|There', 'without|ask|would', 'without|ask|how', 'without|ask|has', 'without|ask|want', 'without|ask|them', 'without|ask|going', 'without|ask|You', 'without|ask|They', 'without|ask|more', 'without|ask|see', \"without|ask|'ve\", 'without|ask|he', 'without|ask|It', 'without|ask|do', 'without|ask|\"', 'without|ask|are', 'without|ask|I', 'without|ask|<s>', 'without|ask|this', 'without|ask|?', 'without|ask|with', 'without|ask|We', 'without|ask|for', 'without|ask|on', 'without|ask|The', 'without|ask|:', 'without|ask|one', 'without|ask|my', 'without|ask|If', 'without|ask|doing', 'without|ask|life', 'without|ask|three']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "trying future probably ' technology Do data ever water One off part All done used ago Africa left stuff understand five children everything between may enough For looks become ... country getting women Okay quite end three most even come different lot which work over say she many mean Let started 'd day year every story question thought went another too never No example back first does also years Thank thing who but some get would actually us right world my 're are n't of <pad> it in our They like just really out 'm had A well use If\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The|internal|combustion\n",
      "['trying|...|anything', 'trying|...|able', 'trying|...|power', 'trying|...|become', 'trying|...|find', 'trying|...|My', 'trying|...|part', 'trying|...|after', 'trying|...|question', 'trying|...|important', 'trying|...|another', 'trying|...|idea', 'trying|...|him', 'trying|...|She', 'trying|...|example', 'trying|...|bit', 'trying|...|between', 'trying|...|try', 'trying|...|working', 'trying|...|believe', 'trying|...|looking', 'trying|...|place', 'trying|...|ever', 'trying|...|Okay', 'trying|...|hard', 'trying|...|Africa', 'trying|...|OK', 'trying|...|understand', 'trying|...|Is', 'trying|...|number', 'trying|...|answer', 'trying|...|goes', 'trying|...|ago', 'trying|...|old', 'trying|...|came', 'trying|...|few', 'trying|...|little', 'trying|...|because', 'trying|...|first', 'trying|...|good', 'trying|...|make', 'trying|...|world', 'trying|...|no', 'trying|...|their', 'trying|...|thing', 'trying|...|other', 'trying|...|who', 'trying|...|Well', 'trying|...|made', 'trying|...|;', 'trying|...|down', 'trying|...|well', 'trying|...|called', 'trying|...|many', 'trying|...|three', 'trying|...|even', \"trying|...|'d\", 'trying|...|being', 'trying|...|course', 'trying|...|should', 'trying|...|change', 'trying|...|Because', 'trying|...|put', 'trying|...|went', 'trying|...|things', 'trying|...|or', 'trying|...|now', 'trying|...|then', 'trying|...|had', 'trying|...|were', 'trying|...|did', 'trying|...|What', 'trying|...|people', 'trying|...|my', 'trying|...|one', 'trying|...|This', 'trying|...|know', 'trying|...|They', 'trying|...|these', 'trying|...|our', 'trying|...|we', 'trying|...|in', 'trying|...|of', 'trying|...|is', 'trying|...|,', 'trying|...|<pad>', 'trying|...|the', 'trying|...|to', \"trying|...|n't\", 'trying|...|have', 'trying|...|this', 'trying|...|It', 'trying|...|with', 'trying|...|on', 'trying|...|there', 'trying|...|-', 'trying|...|his', 'trying|...|kind', 'trying|...|back', 'trying|...|life']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "anything bad high may might after few old My bit No example better One its water interesting women end found each looking data ever understand means working everything help picture play number point long before ago life over she called because something years been which good different say These big talk Let A made day through important idea show went thought again start human Thank could their way want did time or our going know -- really up he your on n't ? are <s> <unk> the that But be about with You one very as ; same three why\n",
      "So|planning|has\n",
      "['anything|saw|using', 'anything|saw|space', 'anything|saw|without', 'anything|saw|works', 'anything|saw|working', 'anything|saw|second', 'anything|saw|try', 'anything|saw|between', 'anything|saw|system', 'anything|saw|might', 'anything|saw|better', 'anything|saw|its', 'anything|saw|When', 'anything|saw|before', 'anything|saw|wanted', 'anything|saw|feel', 'anything|saw|light', 'anything|saw|design', 'anything|saw|dollars', 'anything|saw|become', 'anything|saw|bad', 'anything|saw|goes', 'anything|saw|kids', 'anything|saw|power', 'anything|saw|information', 'anything|saw|food', 'anything|saw|getting', 'anything|saw|thinking', 'anything|saw|As', 'anything|saw|create', 'anything|saw|wrong', 'anything|saw|sense', 'anything|saw|For', 'anything|saw|picture', 'anything|saw|OK', 'anything|saw|10', 'anything|saw|life', 'anything|saw|over', 'anything|saw|many', 'anything|saw|come', 'anything|saw|He', 'anything|saw|Thank', 'anything|saw|world', 'anything|saw|will', 'anything|saw|does', 'anything|saw|years', 'anything|saw|In', 'anything|saw|lot', 'anything|saw|idea', 'anything|saw|any', 'anything|saw|day', 'anything|saw|started', 'anything|saw|course', 'anything|saw|made', 'anything|saw|percent', 'anything|saw|should', 'anything|saw|No', 'anything|saw|human', 'anything|saw|question', 'anything|saw|still', 'anything|saw|own', 'anything|saw|My', 'anything|saw|find', 'anything|saw|part', 'anything|saw|much', 'anything|saw|way', 'anything|saw|or', 'anything|saw|their', 'anything|saw|has', 'anything|saw|he', 'anything|saw|us', 'anything|saw|then', 'anything|saw|these', 'anything|saw|know', 'anything|saw|from', 'anything|saw|so', 'anything|saw|really', 'anything|saw|them', 'anything|saw|were', \"anything|saw|'ve\", 'anything|saw|it', 'anything|saw|you', 'anything|saw|that', 'anything|saw|And', 'anything|saw|a', 'anything|saw|to', 'anything|saw|I', 'anything|saw|is', \"anything|saw|'re\", 'anything|saw|The', 'anything|saw|this', \"anything|saw|n't\", 'anything|saw|This', 'anything|saw|there', 'anything|saw|my', 'anything|saw|like', 'anything|saw|;', 'anything|saw|great', 'anything|saw|same', 'anything|saw|If']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "ask future thinking ' together technology each ever water part bit find came done used long play For Africa picture happens simple means understand power person looks answer design dollars light getting working children sort live into other thing who much said then things will if Thank when come around most down different lot say many mean today ; percent idea ca start never but What by how more Now had he so from just me up -- see think and in is that </s> <pad> the a can on ? what The be 're like where work also 'll\n",
      "The|most|important\n",
      "['ask|trying|thinking', 'ask|trying|future', 'ask|trying|probably', 'ask|trying|coming', 'ask|trying|technology', 'ask|trying|Do', 'ask|trying|together', 'ask|trying|happened', 'ask|trying|part', 'ask|trying|off', 'ask|trying|him', 'ask|trying|example', 'ask|trying|brain', 'ask|trying|One', 'ask|trying|always', 'ask|trying|ago', 'ask|trying|For', 'ask|trying|Africa', 'ask|trying|stuff', 'ask|trying|10', 'ask|trying|school', 'ask|trying|children', 'ask|trying|am', 'ask|trying|OK', 'ask|trying|person', 'ask|trying|bad', 'ask|trying|play', 'ask|trying|number', 'ask|trying|else', 'ask|trying|may', 'ask|trying|...', 'ask|trying|become', 'ask|trying|second', 'ask|trying|end', 'ask|trying|data', 'ask|trying|four', 'ask|trying|his', 'ask|trying|she', 'ask|trying|many', 'ask|trying|called', 'ask|trying|because', 'ask|trying|Well', 'ask|trying|Thank', 'ask|trying|make', 'ask|trying|which', 'ask|trying|first', 'ask|trying|over', 'ask|trying|only', 'ask|trying|today', \"ask|trying|'d\", 'ask|trying|problem', 'ask|trying|being', 'ask|trying|percent', 'ask|trying|course', 'ask|trying|should', 'ask|trying|every', 'ask|trying|important', 'ask|trying|idea', 'ask|trying|put', 'ask|trying|another', 'ask|trying|start', 'ask|trying|question', 'ask|trying|still', 'ask|trying|human', 'ask|trying|no', 'ask|trying|said', 'ask|trying|There', 'ask|trying|things', 'ask|trying|now', 'ask|trying|What', 'ask|trying|but', 'ask|trying|time', 'ask|trying|out', 'ask|trying|up', 'ask|trying|these', 'ask|trying|them', 'ask|trying|were', 'ask|trying|think', 'ask|trying|more', 'ask|trying|he', 'ask|trying|have', 'ask|trying|It', 'ask|trying|and', 'ask|trying|this', 'ask|trying|<s>', 'ask|trying|<unk>', 'ask|trying|of', 'ask|trying|it', 'ask|trying|at', 'ask|trying|all', 'ask|trying|:', 'ask|trying|The', 'ask|trying|They', 'ask|trying|people', 'ask|trying|an', 'ask|trying|know', 'ask|trying|well', 'ask|trying|down', 'ask|trying|How', 'ask|trying|even']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "dollars able best future together technology wanted looking done water part next ago before place million simple everything live children found feel women pretty help means comes money goes looks bad enough Okay ever data whole kind say most How something other will two than Well good different any Here put show through year 'd change thought give idea important human question him example There time but us by your how has our so at my were up 've he do are in \" </s> <unk> . a But about It We - 're one This course great down let\n",
      "Just|go|and\n",
      "['dollars|...|country', 'dollars|...|design', 'dollars|...|ask', 'dollars|...|talking', 'dollars|...|came', 'dollars|...|done', 'dollars|...|might', 'dollars|...|system', 'dollars|...|him', 'dollars|...|too', 'dollars|...|thought', 'dollars|...|start', 'dollars|...|find', 'dollars|...|example', 'dollars|...|better', 'dollars|...|real', 'dollars|...|comes', 'dollars|...|am', 'dollars|...|away', 'dollars|...|happen', 'dollars|...|together', 'dollars|...|long', 'dollars|...|quite', 'dollars|...|found', 'dollars|...|hard', 'dollars|...|left', 'dollars|...|OK', 'dollars|...|understand', 'dollars|...|answer', 'dollars|...|looks', 'dollars|...|able', 'dollars|...|dollars', 'dollars|...|place', 'dollars|...|When', 'dollars|...|few', 'dollars|...|used', 'dollars|...|who', 'dollars|...|thing', \"dollars|...|'ll\", 'dollars|...|which', 'dollars|...|much', 'dollars|...|no', 'dollars|...|has', 'dollars|...|said', 'dollars|...|Thank', 'dollars|...|if', 'dollars|...|got', 'dollars|...|been', 'dollars|...|through', 'dollars|...|story', 'dollars|...|her', 'dollars|...|A', 'dollars|...|let', 'dollars|...|three', 'dollars|...|If', 'dollars|...|course', 'dollars|...|mean', \"dollars|...|'d\", 'dollars|...|started', 'dollars|...|!', 'dollars|...|big', 'dollars|...|Because', 'dollars|...|change', 'dollars|...|These', 'dollars|...|how', 'dollars|...|by', 'dollars|...|were', 'dollars|...|more', 'dollars|...|out', 'dollars|...|That', 'dollars|...|see', 'dollars|...|Now', 'dollars|...|so', 'dollars|...|from', 'dollars|...|very', 'dollars|...|just', 'dollars|...|these', 'dollars|...|know', 'dollars|...|our', 'dollars|...|them', 'dollars|...|and', 'dollars|...|you', 'dollars|...|is', 'dollars|...|And', 'dollars|...|the', 'dollars|...|,', 'dollars|...|to', 'dollars|...|of', 'dollars|...|It', 'dollars|...|are', 'dollars|...|\"', 'dollars|...|So', 'dollars|...|The', 'dollars|...|:', 'dollars|...|at', 'dollars|...|You', 'dollars|...|How', 'dollars|...|she', 'dollars|...|where', 'dollars|...|life']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "Africa comes picture help find off its after question idea put went still start No him four whole technology wanted place always Do long live away ever feel try interesting happen am All old done system also who Well first no said There or much way could thing course well let use doing most new why tell day year should 'd talk big change but some get now your 've by did from an very They here our 'm had are So this ? </s> <unk> a of with not for We there 're all This many those than good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The|book|was\n",
      "['Africa|bad|making', 'Africa|bad|times', 'Africa|bad|nothing', 'Africa|bad|less', 'Africa|bad|children', 'Africa|bad|live', 'Africa|bad|interesting', 'Africa|bad|working', 'Africa|bad|wanted', 'Africa|bad|together', 'Africa|bad|long', 'Africa|bad|million', 'Africa|bad|feel', 'Africa|bad|ever', 'Africa|bad|second', 'Africa|bad|pretty', 'Africa|bad|else', 'Africa|bad|high', 'Africa|bad|bad', 'Africa|bad|person', 'Africa|bad|left', 'Africa|bad|understand', 'Africa|bad|answer', 'Africa|bad|goes', 'Africa|bad|future', 'Africa|bad|country', 'Africa|bad|best', 'Africa|bad|light', 'Africa|bad|coming', 'Africa|bad|trying', 'Africa|bad|Then', 'Africa|bad|As', 'Africa|bad|stuff', 'Africa|bad|means', 'Africa|bad|took', 'Africa|bad|between', 'Africa|bad|year', 'Africa|bad|well', 'Africa|bad|should', 'Africa|bad|tell', 'Africa|bad|three', 'Africa|bad|many', 'Africa|bad|different', 'Africa|bad|say', 'Africa|bad|new', 'Africa|bad|even', 'Africa|bad|same', 'Africa|bad|use', 'Africa|bad|own', 'Africa|bad|off', 'Africa|bad|Why', 'Africa|bad|example', 'Africa|bad|again', 'Africa|bad|another', 'Africa|bad|thought', 'Africa|bad|question', 'Africa|bad|real', 'Africa|bad|water', 'Africa|bad|find', 'Africa|bad|better', 'Africa|bad|before', 'Africa|bad|few', 'Africa|bad|When', 'Africa|bad|ago', 'Africa|bad|lot', 'Africa|bad|work', 'Africa|bad|In', 'Africa|bad|than', 'Africa|bad|much', 'Africa|bad|way', 'Africa|bad|got', 'Africa|bad|also', 'Africa|bad|That', 'Africa|bad|up', 'Africa|bad|--', 'Africa|bad|these', 'Africa|bad|did', 'Africa|bad|had', 'Africa|bad|then', 'Africa|bad|go', 'Africa|bad|So', 'Africa|bad|?', 'Africa|bad|And', 'Africa|bad|we', 'Africa|bad|.', 'Africa|bad|<s>', 'Africa|bad|a', 'Africa|bad|is', 'Africa|bad|:', 'Africa|bad|with', 'Africa|bad|have', 'Africa|bad|they', \"Africa|bad|'re\", 'Africa|bad|The', 'Africa|bad|very', 'Africa|bad|They', 'Africa|bad|Here', 'Africa|bad|went', 'Africa|bad|mean', 'Africa|bad|put']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "hand times nothing says technology million whole women better fact example bit real after done ago left stuff OK means between simple am money best kids hard goes home become Then moment believe school second children doing take around same than look need because only over life kind went big ! Because tell story problem being question thought idea again still start too him something years will when right their much world but some by would time actually There no all not It about a <pad> is and That up like They were see had he her great use If\n",
      "Because|,|you\n",
      "['hand|without|energy', 'hand|without|works', 'hand|without|told', 'hand|without|yet', 'hand|without|pretty', 'hand|without|end', 'hand|without|school', 'hand|without|simple', 'hand|without|wanted', 'hand|without|million', 'hand|without|ago', 'hand|without|long', 'hand|without|whole', 'hand|without|looking', 'hand|without|ever', 'hand|without|quite', 'hand|without|getting', 'hand|without|light', 'hand|without|may', 'hand|without|else', 'hand|without|Is', 'hand|without|play', 'hand|without|bad', 'hand|without|high', 'hand|without|create', 'hand|without|coming', 'hand|without|future', 'hand|without|food', 'hand|without|guy', 'hand|without|wrong', 'hand|without|making', 'hand|without|less', 'hand|without|For', 'hand|without|stuff', 'hand|without|took', 'hand|without|between', 'hand|without|made', 'hand|without|come', 'hand|without|percent', 'hand|without|should', \"hand|without|'ll\", 'hand|without|those', 'hand|without|In', 'hand|without|little', 'hand|without|lot', 'hand|without|work', 'hand|without|How', 'hand|without|three', 'hand|without|him', 'hand|without|too', 'hand|without|still', 'hand|without|She', 'hand|without|another', 'hand|without|any', 'hand|without|question', 'hand|without|Why', 'hand|without|own', 'hand|without|fact', 'hand|without|bit', 'hand|without|love', 'hand|without|real', 'hand|without|One', 'hand|without|before', 'hand|without|When', 'hand|without|also', 'hand|without|something', 'hand|without|Thank', 'hand|without|been', 'hand|without|then', 'hand|without|some', 'hand|without|no', 'hand|without|world', 'hand|without|out', 'hand|without|up', 'hand|without|--', 'hand|without|our', 'hand|without|your', \"hand|without|'ve\", 'hand|without|did', 'hand|without|get', 'hand|without|they', \"hand|without|n't\", 'hand|without|\"', 'hand|without|It', 'hand|without|</s>', 'hand|without|<pad>', 'hand|without|a', \"hand|without|'s\", 'hand|without|be', 'hand|without|not', 'hand|without|about', 'hand|without|can', 'hand|without|You', 'hand|without|But', 'hand|without|an', 'hand|without|from', 'hand|without|These', 'hand|without|ca', 'hand|without|Because', 'hand|without|big']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "food thinking makes probably wanted together data feel might water find One before system used place bad goes play looks Africa understand picture For best may kids high design become getting future comes between children interesting around new why well than Well could two different 'll take down Here put big change tell every started ! human never important give love too fact own world much or right has would us actually Now think That see 've more your by about We for they <s> <unk> And have - The : But just one an from day A course percent\n",
      "My|family|believes\n",
      "['food|small|anything', 'food|small|able', 'food|small|high', 'food|small|may', 'food|small|example', 'food|small|No', 'food|small|own', 'food|small|find', 'food|small|any', 'food|small|show', 'food|small|change', 'food|small|put', 'food|small|give', 'food|small|another', 'food|small|question', 'food|small|still', 'food|small|live', 'food|small|women', 'food|small|ever', 'food|small|feel', 'food|small|wanted', 'food|small|old', 'food|small|point', 'food|small|data', 'food|small|money', 'food|small|between', 'food|small|five', 'food|small|working', 'food|small|help', 'food|small|OK', 'food|small|answer', 'food|small|bad', 'food|small|might', 'food|small|brain', 'food|small|real', 'food|small|done', 'food|small|because', 'food|small|In', 'food|small|little', 'food|small|than', 'food|small|right', 'food|small|no', 'food|small|us', 'food|small|There', 'food|small|world', 'food|small|way', 'food|small|years', 'food|small|been', 'food|small|percent', 'food|small|course', 'food|small|why', 'food|small|same', 'food|small|his', 'food|small|she', 'food|small|most', 'food|small|new', 'food|small|through', 'food|small|story', 'food|small|should', 'food|small|day', 'food|small|today', 'food|small|being', 'food|small|Because', 'food|small|big', 'food|small|but', 'food|small|want', 'food|small|would', 'food|small|has', 'food|small|were', 'food|small|going', 'food|small|how', 'food|small|get', 'food|small|like', 'food|small|my', 'food|small|one', 'food|small|at', 'food|small|people', 'food|small|just', 'food|small|as', 'food|small|these', 'food|small|\"', 'food|small|this', 'food|small|you', 'food|small|it', 'food|small|I', 'food|small|<pad>', 'food|small|is', 'food|small|And', 'food|small|can', 'food|small|about', 'food|small|It', \"food|small|n't\", 'food|small|be', 'food|small|not', \"food|small|'re\", 'food|small|all', 'food|small|kind', 'food|small|say', 'food|small|back', \"food|small|'ll\"]\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "found quite children took Here went again thought being problem story tell Because started ca put always few came might real fact done brain long Do When place wanted million data ever off No still never been when into need There time he your go or their right new How many called only say she his great let why come percent her year every 've were them up me an from going there 're be But at one people as \" in And it a <pad> of 's n't for do have on they We : than little also Well\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ceaefa2e72d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#print (' '.join([EN.vocab.itos[id.data[0]] for id in decode_str[1:]]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0manswer_token\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecode_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_kaggle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-5437c4ee0c07>\u001b[0m in \u001b[0;36mevaluate_kaggle\u001b[0;34m(encoder, decoder, string, k, ngrams, max_length, batch_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mcurr_word_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopk_word_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mcurr_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopk_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "match = 0 \n",
    "total = 0 \n",
    "top_match = 0\n",
    "for batch in iter(val_iter): \n",
    "    for t in range(batch.src.size()[1]): \n",
    "        string = batch.src[:,t]\n",
    "        decode_str = batch.trg[:, t]\n",
    "        #print (' '.join([DE.vocab.itos[id.data[0]] for id in string]))\n",
    "        #print (' '.join([EN.vocab.itos[id.data[0]] for id in decode_str[1:]]))\n",
    "        answer_token ='|'.join([EN.vocab.itos[id.data[0]] for id in decode_str[1:4]])\n",
    "        output_tokens = evaluate_kaggle(encoder2.cuda(), decoder2.cuda(), string, k = 100, ngrams = 3, batch_size=100).split(\" \")\n",
    "        print(answer_token)\n",
    "        print(output_tokens)\n",
    "        if answer_token in output_tokens: \n",
    "            match += 1 \n",
    "            if answer_token in output_tokens[:3]: \n",
    "                top_match += 1 \n",
    "        total += 1 \n",
    "    print(top_match/total)\n",
    "    print(match/total)\n",
    "print(\"accuracy: \", match/total)\n",
    "#64: 0.46875, 0.65625 #128 0.4609 0.633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_weight = Variable(torch.FloatTensor(len(EN.vocab)).fill_(1))\n",
    "mask_weight[PAD_token] = 0\n",
    "mask_weight = mask_weight.cuda() if use_cuda else mask_weight\n",
    "criterion = nn.NLLLoss(weight=mask_weight, size_average=False)\n",
    "val_loss = validate(encoder1, decoder1, val_iter, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: sizes do not match at /pytorch/torch/lib/THC/THCTensorCopy.cu:31",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fdafa4f9c994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m#print (' '.join([EN.vocab.itos[id.data[0]] for id in decode_str[1:]]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0manswer_token\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecode_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_kaggle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1b8ccf3550ec>\u001b[0m in \u001b[0;36mevaluate_kaggle\u001b[0;34m(encoder, decoder, string, k, ngrams, max_length, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output_short\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#decoder_input = Variable(torch.ones(1, batch_length).long()*SOS_token)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mMaskedFill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSetItem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, index, value)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvanced_indexing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_preprocess_adv_index_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: sizes do not match at /pytorch/torch/lib/THC/THCTensorCopy.cu:31"
     ]
    }
   ],
   "source": [
    "# bidir models\n",
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "PAD_token = 1 \n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, h_size, batch_size, n_layers=1, dropout=0, bidir=False):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.num_layers = n_layers\n",
    "        self.hidden_size = h_size\n",
    "        self.batch_size = batch_size\n",
    "        self.bidir=bidir\n",
    "        self.embed = nn.Embedding(input_size, h_size)\n",
    "        self.lstm = nn.LSTM(h_size, h_size, dropout=dropout, num_layers=n_layers, bidirectional=bidir)\n",
    "\n",
    "    def forward(self, input_src, hidden):\n",
    "        embedded = self.embed(input_src)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.bidir: \n",
    "            bi_dir_layers  = 2\n",
    "        else: \n",
    "            bi_dir_layers  = 1\n",
    "        result = (Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)),\n",
    "                  Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)))\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)).cuda(),\n",
    "                    Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)).cuda())\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        # hidden -> target_len x batch_size x hidden_dim\n",
    "        hidden = hidden.transpose(0, 1) # batch_size x target_len x hidden_dim\n",
    "        \n",
    "        # encoder_outputs -> max_len x batch_size x hidden_dim\n",
    "        encoder_outputs = encoder_outputs.permute(1, 2, 0)\n",
    "        \n",
    "        attn_energies = torch.bmm(hidden, encoder_outputs) # B x S\n",
    "        \n",
    "\n",
    "        return F.softmax(attn_energies, dim=2)\n",
    "        \n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size, dropout=0.1, n_layers=1, max_length=MAX_LEN):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size*2\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = Attn(hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "\n",
    "    def forward(self, input_data, hidden, encoder_outputs):\n",
    "        #input_len x batch_size \n",
    "\n",
    "        embedded = self.embedding(input_data) #batch_size x target_len x hidden dim\n",
    "\n",
    "        #lstm_output -> target_len x batch_size x hidden_dim\n",
    "        lstm_output, lstm_hidden = self.lstm(embedded, hidden)\n",
    "\n",
    "        #attn input 0 to T-1 \n",
    "        if hidden[0].size()[0] != 1: \n",
    "            attn_hidden = hidden[0][-1].unsqueeze(0)\n",
    "        else: \n",
    "            attn_hidden = hidden[0]\n",
    "        \n",
    "        if(lstm_output.size()[0] > 1):  \n",
    "            attn_input = torch.cat((attn_hidden, lstm_output[:-1]))\n",
    "        else: \n",
    "            attn_input = attn_hidden\n",
    "        # encoder_outputs -> max_len x batch_size x hidden_dim\n",
    "        attn_weights = self.attn(attn_input, encoder_outputs)\n",
    "        \n",
    "        # context = batch_size x target_length x hidden_dim \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) \n",
    "        \n",
    "        context = context.transpose(1, 0) #target_length x batch_size x hidden_dim\n",
    "        \n",
    "        output = torch.cat((lstm_output, context), 2)\n",
    "\n",
    "        # Final output layer\n",
    "        final_output = F.log_softmax(self.out(output), dim=2)\n",
    "        final_output = self.dropout(final_output)\n",
    "        return final_output, lstm_hidden, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        result = (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                  Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)))\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)).cuda(),\n",
    "                    Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)).cuda())\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "def train_batch(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LEN):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    #useful lengths \n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    batch_size = target_variable.size()[1]\n",
    "    layers = encoder.num_layers\n",
    "    # zero words and zero loss \n",
    "    loss = 0 \n",
    "    total_words = 0 \n",
    "    encoder_output_short, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    encoder_outputs = Variable(torch.zeros(max_length, batch_size, 2*encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "    \n",
    "    #ATTENTION\n",
    "    encoder_outputs[:input_length, :, :] = encoder_output_short \n",
    "\n",
    "    if layers != 1: \n",
    "        decoder_hidden = (torch.cat((encoder_hidden[0][-layers:], encoder_hidden[0][-layers:]), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][-layers:], encoder_hidden[1][-layers:]), dim=2)) \n",
    "    else: \n",
    "         decoder_hidden = (torch.cat((encoder_hidden[0][0].unsqueeze(0), encoder_hidden[0][1].unsqueeze(0)), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][0].unsqueeze(0), encoder_hidden[1][1].unsqueeze(0)), dim=2)) \n",
    "        \n",
    "    #using encoder output and target variable \n",
    "\n",
    "    decoder_output, decoder_hidden, decoder_attention = decoder(target_variable, decoder_hidden, encoder_outputs)\n",
    "    \n",
    "    #initialize last row as padding tokens\n",
    "    last_row = torch.ones(1, batch_size).long()\n",
    "    last_row = last_row.cuda() if use_cuda else last_row\n",
    "    \n",
    "    #shift target from 1:n + padding row\n",
    "    shifted_target = Variable(torch.cat((target_variable[1:, :].data.long(), last_row)))\n",
    "    m, i = torch.max(decoder_output, dim=2)\n",
    "\n",
    "    #calculate decoder_output loss with shifted target loss\n",
    "\n",
    "    loss = criterion(decoder_output.view(target_length*batch_size, -1), shifted_target.view(target_length*batch_size))\n",
    "    # count total words\n",
    "    total_words = shifted_target.ne(PAD_token).int().sum()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), 3.0)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 3.0)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.data[0]/total_words.data[0]\n",
    "\n",
    "\n",
    "def validate(encoder, decoder, val_iter, criterion, max_length = MAX_LEN):\n",
    "    encoder.eval() \n",
    "    decoder.eval() \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    total_words = 0 \n",
    "    for batch in iter(val_iter):\n",
    "        num_batches += 1 \n",
    "        input_length = batch.src.size()[0]\n",
    "        target_length = batch.trg.size()[0]\n",
    "        batch_size = batch.src.size()[1]\n",
    "        layers = encoder.num_layers\n",
    "        if batch_size != 64:\n",
    "            break\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "         \n",
    "        encoder_output_short, encoder_hidden = encoder(batch.src, encoder_hidden)\n",
    "        encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size*2))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "        encoder_outputs[:input_length, :, :] = encoder_output_short      \n",
    "\n",
    "        if layers != 1: \n",
    "            decoder_hidden = (torch.cat((encoder_hidden[0][-layers:], encoder_hidden[0][-layers:]), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][-layers:], encoder_hidden[1][-layers:]), dim=2)) \n",
    "        else: \n",
    "            decoder_hidden = (torch.cat((encoder_hidden[0][0].unsqueeze(0), encoder_hidden[0][1].unsqueeze(0)), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][0].unsqueeze(0), encoder_hidden[1][1].unsqueeze(0)), dim=2)) \n",
    "        \n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(batch.trg, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        m, i = torch.max(decoder_output, dim=2)\n",
    "        plot_attention(decoder_attention[0, :, :].data.cpu().numpy(), batch.src.data[:, 0].cpu().numpy(), i.data[:, 0].cpu().numpy())\n",
    "                    \n",
    "        first_row = torch.ones(1, batch_size).long()\n",
    "        first_row = first_row.cuda() if use_cuda else first_row\n",
    "        \n",
    "        shifted_target = Variable(torch.cat((batch.trg[1:, :].data.long(), first_row)))\n",
    "        loss = criterion(decoder_output.view(target_length*batch_size, -1), shifted_target.view(target_length*batch_size))\n",
    "        total_words += shifted_target.ne(PAD_token).int().sum()\n",
    "\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "    return total_loss / total_words.data[0]\n",
    "\n",
    "def plot_attention(d_attn, source_seq, target_seq):  \n",
    "    fig = plt.figure()\n",
    "    eos_ind = list(target_seq).index(EOS_token)\n",
    "    d_attn = d_attn[:len(source_seq), :eos_ind]\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(d_attn, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    source_sentence = [DE.vocab.itos[j] for j in source_seq]\n",
    "\n",
    "    target_sentence = [EN.vocab.itos[j] for j in target_seq]\n",
    "    print(source_sentence)\n",
    "    print(target_sentence)\n",
    "    ax.set_xticklabels([''] + source_sentence, rotation=90)\n",
    "    ax.set_yticklabels([''] + target_sentence)\n",
    "\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def trainIters(encoder, decoder, training_iter, valid_iter, target_vocab_len, learning_rate=0.7, num_epochs=20):\n",
    "    encoder.train() \n",
    "    decoder.train() \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    #mask weight to not consider in loss \n",
    "    mask_weight = Variable(torch.FloatTensor(target_vocab_len).fill_(1))\n",
    "    mask_weight[PAD_token] = 0\n",
    "    mask_weight = mask_weight.cuda() if use_cuda else mask_weight\n",
    "    \n",
    "    #pass mask weight in to NLL Loss without size_average\n",
    "    criterion = nn.NLLLoss(weight=mask_weight, size_average=False)\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        #initialise total loss and batch count\n",
    "        batch_len = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in iter(training_iter):\n",
    "            if batch.src.size()[1] == 64: \n",
    "                loss = train_batch(batch.src, batch.trg, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "                total_loss += loss\n",
    "                batch_len += 1\n",
    "            \n",
    "        # divide total loss by batch_length\n",
    "        train_loss = total_loss / batch_len\n",
    "        \n",
    "        print(\"train loss: \", train_loss)\n",
    "        print(\"train ppl: \", np.exp(train_loss))\n",
    "        val_loss = validate(encoder, decoder, valid_iter, criterion)\n",
    "        print(\"val loss: \", val_loss)\n",
    "        print(\"val ppl: \", np.exp(val_loss))\n",
    "\n",
    "match = 0 \n",
    "total = 0 \n",
    "top_match = 0\n",
    "for batch in iter(val_iter): \n",
    "    for t in range(batch.src.size()[1]): \n",
    "        string = batch.src[:,t]\n",
    "        decode_str = batch.trg[:, t]\n",
    "        #print (' '.join([DE.vocab.itos[id.data[0]] for id in string]))\n",
    "        #print (' '.join([EN.vocab.itos[id.data[0]] for id in decode_str[1:]]))\n",
    "        answer_token ='|'.join([EN.vocab.itos[id.data[0]] for id in decode_str[1:4]])\n",
    "        output_tokens = evaluate_kaggle(encoder2.cuda(), decoder2.cuda(), string, k = 100, ngrams = 3, batch_size=100).split(\" \")\n",
    "        print(answer_token)\n",
    "        print(output_tokens)\n",
    "        if answer_token in output_tokens: \n",
    "            match += 1 \n",
    "            if answer_token in output_tokens[:3]: \n",
    "                top_match += 1 \n",
    "        total += 1 \n",
    "    print(top_match/total)\n",
    "    print(match/total)\n",
    "print(\"accuracy: \", match/total)\n",
    "#64: 0.46875, 0.65625 #128 0.4609 0.633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('source_test.txt', 'r') as fp: \n",
    "    lines = fp.readlines()\n",
    "    \n",
    "print(len(lines))\n",
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")\n",
    "\n",
    "with open('sample1.txt', 'w') as fp: \n",
    "    fp.write('id,word\\n')\n",
    "    for i in range(len(lines)): \n",
    "        if (i%100 == 0): \n",
    "            print(i)\n",
    "        line = lines[i]\n",
    "        tokens = line.strip(\"\\n\").split(\" \")\n",
    "        input_index = [DE.vocab.stoi[t] for t in tokens]\n",
    "        input_index = Variable(torch.Tensor((input_index)).long().cuda())\n",
    "        output_str = evaluate_kaggle(encoder2.cuda(), decoder2.cuda(), input_index, k = 100, ngrams = 3, batch_size=100)\n",
    "        output_str = escape(output_str)\n",
    "        fp.write(str(i+1) + ',' + output_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample1.txt', 'r') as fp: \n",
    "    lines = fp.readlines()\n",
    "    with open('sample2.txt', 'w') as wp:\n",
    "        wp.write('id,word\\n')\n",
    "        for i in range(1, len(lines)): \n",
    "            line=lines[i]\n",
    "            tokens = line.split(\",\")\n",
    "            print(tokens)\n",
    "            wp.write(str(i) + ',' + tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw3-s18/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id,word\n",
    "1,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "2,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "3,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "4,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. (In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.)\n",
    "\n",
    "Finally, as always please put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
