{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to process the raw data using a tokenizer. We are going to be using spacy, which can be installed via:  \n",
    "  `[sudo] pip install spacy`  \n",
    "  \n",
    "Tokenizers for English/German can be installed via:  \n",
    "  `[sudo] python -m spacy download en`  \n",
    "  `[sudo] python -m spacy download de`\n",
    "  \n",
    "This isn't *strictly* necessary, and you can use your own tokenization rules if you prefer (e.g. a simple `split()` in addition to some rules to acccount for punctuation), but we recommend sticking to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to add the beginning-of-sentence token `<s>` and the end-of-sentence token `</s>` to the \n",
    "target so we know when to begin/end translating. We do not need to do this on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data. This may take a few minutes.\n",
    "\n",
    "**While this dataset of 200K sentence pairs is relatively small compared to others, it will still take some time to train. So we are going to be only working with sentences of length at most 20 for this homework. Please train only on this reduced dataset for this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary and convert the text corpus into indices. We are going to be replacing tokens that occurred less than 5 times with `<unk>` tokens, and take the rest as our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our data into batches as usual. Batching for MT is slightly tricky because source/target will be of different lengths. Fortunately, `torchtext` lets you do this by allowing you to pass in a `sort_key` function. This will minimizing the amount of padding on the source side, but since there is still some padding you will inadvertendly \"attend\" to these padding tokens. \n",
    "\n",
    "One way to get rid of padding is to pass a binary `mask` vector to your attention module so its attention score (before the softmax) is minus infinity for the padding token. Another way (which is how we do it for our projects, e.g. opennmt) is to manually sort data into batches so that each batch has exactly the same source length (this means that some batches will be less than the desired batch size, though).\n",
    "\n",
    "However, for this homework padding won't matter too much, so it's fine to ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=0,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see that the BOS/EOS token is indeed appended to the target (English) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "print(\"Target\")\n",
    "print(batch.trg)\n",
    "print(batch.src.volatile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now that we've processed the data, we are ready to begin modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': <torchtext.data.field.Field object at 0x7f6f9c785550>, 'trg': <torchtext.data.field.Field object at 0x7f6f9c785780>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n",
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13353\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Set up \n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS\n",
    "\n",
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))\n",
    "\n",
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "# print(DE.vocab.stoi[\"<s>\"], DE.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>\n",
    "print(EN.vocab.stoi[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(DE.vocab.stoi[\"<pad>\"])\n",
    "BATCH_SIZE = 64\n",
    "if use_cuda: \n",
    "    train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=0,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))\n",
    "else: \n",
    "    train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Source\n",
      "Variable containing:\n",
      "    23     26    589  ...     481    218     12\n",
      "     4      4    320  ...     282     59    503\n",
      "   130     19     13  ...    1371     11      9\n",
      "        ...            â‹±           ...         \n",
      "    69      0      3  ...    1062      0      5\n",
      "   154      0   1933  ...       4   1167   1898\n",
      "     2      2     16  ...       2      2      2\n",
      "[torch.cuda.LongTensor of size 16x64 (GPU 0)]\n",
      "\n",
      "1861\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "#     print(\"Target\")\n",
    "#print(batch.trg)\n",
    "print(len(list(train_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidir models\n",
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "PAD_token = 1 \n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, h_size, batch_size, n_layers=1, dropout=0, bidir=False):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.num_layers = n_layers\n",
    "        self.hidden_size = h_size\n",
    "        self.batch_size = batch_size\n",
    "        self.bidir=bidir\n",
    "        self.embed = nn.Embedding(input_size, h_size)\n",
    "        self.lstm = nn.LSTM(h_size, h_size, dropout=dropout, num_layers=n_layers, bidirectional=bidir)\n",
    "\n",
    "    def forward(self, input_src, hidden):\n",
    "        embedded = self.embed(input_src)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.bidir: \n",
    "            bi_dir_layers  = 2\n",
    "        else: \n",
    "            bi_dir_layers  = 1\n",
    "        result = (Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)),\n",
    "                  Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)))\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)).cuda(),\n",
    "                    Variable(torch.zeros(self.num_layers*bi_dir_layers, self.batch_size, self.hidden_size)).cuda())\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        # hidden -> target_len x batch_size x hidden_dim\n",
    "        hidden = hidden.transpose(0, 1) # batch_size x target_len x hidden_dim\n",
    "        \n",
    "        # encoder_outputs -> max_len x batch_size x hidden_dim\n",
    "        encoder_outputs = encoder_outputs.permute(1, 2, 0)\n",
    "        \n",
    "        attn_energies = torch.bmm(hidden, encoder_outputs) # B x S\n",
    "        \n",
    "\n",
    "        return F.softmax(attn_energies, dim=2)\n",
    "        \n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size, dropout=0.1, n_layers=1, max_length=MAX_LEN):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size*2\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = Attn(hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "\n",
    "    def forward(self, input_data, hidden, encoder_outputs):\n",
    "        #input_len x batch_size \n",
    "\n",
    "        embedded = self.embedding(input_data) #batch_size x target_len x hidden dim\n",
    "\n",
    "        #lstm_output -> target_len x batch_size x hidden_dim\n",
    "        lstm_output, lstm_hidden = self.lstm(embedded, hidden)\n",
    "\n",
    "        #attn input 0 to T-1 \n",
    "        if hidden[0].size()[0] != 1: \n",
    "            attn_hidden = hidden[0][-1].unsqueeze(0)\n",
    "        else: \n",
    "            attn_hidden = hidden[0]\n",
    "        \n",
    "        if(lstm_output.size()[0] > 1):  \n",
    "            attn_input = torch.cat((attn_hidden, lstm_output[:-1]))\n",
    "        else: \n",
    "            attn_input = attn_hidden\n",
    "        # encoder_outputs -> max_len x batch_size x hidden_dim\n",
    "        attn_weights = self.attn(attn_input, encoder_outputs)\n",
    "        \n",
    "        # context = batch_size x target_length x hidden_dim \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) \n",
    "        \n",
    "        context = context.transpose(1, 0) #target_length x batch_size x hidden_dim\n",
    "        \n",
    "        output = torch.cat((lstm_output, context), 2)\n",
    "\n",
    "        # Final output layer\n",
    "        final_output = F.log_softmax(self.out(output), dim=2)\n",
    "        final_output = self.dropout(final_output)\n",
    "        return final_output, lstm_hidden, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        result = (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                  Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)))\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)).cuda(),\n",
    "                    Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)).cuda())\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(beams, k):\n",
    "    \"\"\"\n",
    "    Prunes all but the top k beams, by summative score\n",
    "    \"\"\"\n",
    "    beams.sort(key=lambda x: x[1], reverse=True) #sort beams by second element (score)\n",
    "    return beams[:k] #return top k\n",
    "\n",
    "def evaluate_kaggle(encoder, decoder, string, k = 3, ngrams = 3, max_length = 20, batch_size=1):\n",
    "    # Run string through encoder\n",
    "\n",
    "    encoder_input = string.unsqueeze(1).expand(-1, batch_size)\n",
    "\n",
    "    layers = encoder.num_layers\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_output_short, encoder_hidden = encoder(encoder_input, encoder_hidden)\n",
    "    \n",
    "    #expand encoder outputs\n",
    "    input_length = string.size()[0]\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size*2))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    " \n",
    "    print(encoder_output_short.shape)\n",
    "    print(encoder_outputs.shape)\n",
    "    encoder_outputs[:input_length, :, :] = encoder_output_short\n",
    "    \n",
    "    #decoder_input = Variable(torch.ones(1, batch_length).long()*SOS_token)\n",
    "\n",
    "    decoder_input = Variable(torch.ones(1, batch_size).long()*SOS_token) #1 x batch_length\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    if layers != 1: \n",
    "        decoder_hidden = (torch.cat((encoder_hidden[0][-layers:], encoder_hidden[0][-layers:]), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][-layers:], encoder_hidden[1][-layers:]), dim=2)) \n",
    "    else: \n",
    "         decoder_hidden = (torch.cat((encoder_hidden[0][0].unsqueeze(0), encoder_hidden[0][1].unsqueeze(0)), dim=2) , \n",
    "                      torch.cat((encoder_hidden[1][0].unsqueeze(0), encoder_hidden[1][1].unsqueeze(0)), dim=2)) \n",
    "      \n",
    "\n",
    "    # base case - get top k predictions from SOS_token\n",
    "    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "    # Get most likely word index from output\n",
    "    \n",
    "    topk_probs, topk_word_idx = decoder_output.data.topk(k, dim = 2)\n",
    "    print(topk_word_idx[:, 0].shape)\n",
    "    print(' '.join([EN.vocab.itos[id] for id in topk_word_idx[:, 0][0]]))\n",
    "    decoder_input = Variable(topk_word_idx[:, 0]) # Chosen word is next input\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    beam_outputs = [([topk_word_idx.view(-1)[i]], topk_probs.view(-1)[i]) for i in range(k)]\n",
    "    \n",
    "\n",
    "    # non base case\n",
    "    for trg_word_idx in range(0, ngrams - 1): # <s> shouldn't count\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "\n",
    "        # Get most likely word index from output\n",
    "        topk_probs, topk_word_idx = decoder_output.data.topk(k, dim = 2)\n",
    "        \n",
    "        #create top k*k-by-1 matrix\n",
    "        temp_beam = []\n",
    "        for i in range(k):\n",
    "            beam_words = beam_outputs[i][0]\n",
    "            beam_score = beam_outputs[i][1]\n",
    "            for j in range(k):\n",
    "                index = i * k + j\n",
    "                curr_word_index = topk_word_idx.view(-1)[index]\n",
    "                curr_score = topk_probs.view(-1)[index]\n",
    "                \n",
    "                temp_beam.append((beam_words + [curr_word_index], beam_score + curr_score))\n",
    "                \n",
    "        \n",
    "        #prune k*k-by-1 matrix to 1xk\n",
    "        beam_outputs = prune(temp_beam, k)\n",
    "        #print(beam_outputs)\n",
    "        #set beams equal to decoder_input\n",
    "        new_beams = [beam[0] for beam in beam_outputs]\n",
    "\n",
    "        new_beam_input = [[beam[0][-1]] for beam in beam_outputs]\n",
    "        #new_beam_input = new_beams\n",
    "        decoder_input = Variable(torch.LongTensor(new_beam_input)).transpose(0,1) # Chosen beams are next input\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        \n",
    "\n",
    "    #out_idx = zip(*beam_outputs)\n",
    "    kaggle_outputs = ['|'.join([EN.vocab.itos[id] for id in beam]) for beam in new_beams]\n",
    "    return ' '.join(kaggle_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "encoder2 = EncoderLSTM(len(DE.vocab), hidden_size, batch_size=100, dropout=0.3, n_layers=2, bidir=True)\n",
    "decoder2 = AttnDecoderRNN(hidden_size, len(EN.vocab), batch_size=100, dropout=0.3, n_layers=2)\n",
    "\n",
    "encoder2.load_state_dict(torch.load('10_attn_encoder_model.pt'))\n",
    "decoder2.load_state_dict(torch.load('10_attn_decoder_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "stuff OK 10 For water next after done him human thought Why My bit find better end four each whole technology long wanted looking away second Okay women working five try between Do place system few called his most How 'll back thing In where work lot she through problem day tell If come course year today ! being started put ca These show other two Thank when way their world will think these You as want more time things have do \" was of . in and 're But We about - there my like new down even take\n",
      "I|want|to\n",
      "['stuff|end|able', 'stuff|end|number', 'stuff|end|country', 'stuff|end|future', 'stuff|end|mean', 'stuff|end|Let', 'stuff|end|Because', 'stuff|end|change', 'stuff|end|A', 'stuff|end|percent', 'stuff|end|;', 'stuff|end|course', 'stuff|end|story', 'stuff|end|year', 'stuff|end|tell', 'stuff|end|problem', 'stuff|end|point', 'stuff|end|before', 'stuff|end|brain', 'stuff|end|old', 'stuff|end|part', 'stuff|end|find', 'stuff|end|better', 'stuff|end|its', 'stuff|end|women', 'stuff|end|feel', 'stuff|end|happened', 'stuff|end|data', 'stuff|end|school', 'stuff|end|away', 'stuff|end|took', 'stuff|end|happen', 'stuff|end|bit', 'stuff|end|him', 'stuff|end|put', 'stuff|end|thought', 'stuff|end|need', 'stuff|end|who', 'stuff|end|also', 'stuff|end|In', 'stuff|end|when', 'stuff|end|Thank', 'stuff|end|us', 'stuff|end|if', 'stuff|end|He', 'stuff|end|got', 'stuff|end|something', 'stuff|end|thing', 'stuff|end|even', 'stuff|end|How', 'stuff|end|many', 'stuff|end|called', 'stuff|end|over', 'stuff|end|work', 'stuff|end|only', 'stuff|end|his', 'stuff|end|new', 'stuff|end|doing', 'stuff|end|take', 'stuff|end|down', 'stuff|end|use', 'stuff|end|let', 'stuff|end|If', 'stuff|end|great', 'stuff|end|want', 'stuff|end|What', 'stuff|end|your', 'stuff|end|would', 'stuff|end|them', 'stuff|end|our', 'stuff|end|Now', \"stuff|end|'ve\", 'stuff|end|You', 'stuff|end|one', 'stuff|end|be', 'stuff|end|there', 'stuff|end|from', 'stuff|end|just', 'stuff|end|so', 'stuff|end|going', 'stuff|end|in', 'stuff|end|it', 'stuff|end|And', 'stuff|end|you', 'stuff|end|I', 'stuff|end|a', 'stuff|end|is', \"stuff|end|'s\", 'stuff|end|have', 'stuff|end|It', 'stuff|end|this', 'stuff|end|do', 'stuff|end|what', \"stuff|end|n't\", 'stuff|end|We', 'stuff|end|:', 'stuff|end|those', 'stuff|end|look', 'stuff|end|because', 'stuff|end|little']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "may power ... become example human love off show These today Let important any Why never happens working women sort million used happened end looks hard means picture answer number goes able All few next its who other Well first things but had would go said their years well great same If called kind three doing should percent made course talk every started ! think really That out these -- here up people You at my an as me so are So we was <pad> <unk> in and what for It have about We can there say where look work\n",
      "Who|sends|whom\n",
      "['may|country|happening', 'may|country|countries', 'may|country|days', 'may|country|ways', 'may|country|means', 'may|country|money', 'may|country|10', 'may|country|left', 'may|country|interesting', 'may|country|children', 'may|country|data', 'may|country|second', 'may|country|working', 'may|country|five', 'may|country|simple', 'may|country|comes', 'may|country|information', 'may|country|food', 'may|country|country', 'may|country|ask', 'may|country|may', 'may|country|answer', 'may|country|else', 'may|country|...', 'may|country|hand', 'may|country|using', 'may|country|As', 'may|country|social', 'may|country|yet', 'may|country|energy', 'may|country|seen', 'may|country|Or', 'may|country|hard', 'may|country|play', 'may|country|Africa', 'may|country|picture', 'may|country|work', 'may|country|which', 'may|country|where', 'may|country|lot', 'may|country|does', 'may|country|years', 'may|country|Thank', 'may|country|got', 'may|country|Well', 'may|country|need', 'may|country|little', 'may|country|look', 'may|country|human', 'may|country|never', 'may|country|being', 'may|country|went', 'may|country|course', 'may|country|around', 'may|country|her', 'may|country|percent', 'may|country|brain', 'may|country|better', 'may|country|too', 'may|country|example', 'may|country|old', 'may|country|few', 'may|country|before', 'may|country|point', 'may|country|could', 'may|country|will', 'may|country|way', 'may|country|if', 'may|country|no', 'may|country|said', 'may|country|their', 'may|country|right', 'may|country|your', 'may|country|he', \"may|country|'m\", \"may|country|'ve\", 'may|country|has', 'may|country|would', 'may|country|What', 'may|country|There', 'may|country|on', 'may|country|they', 'may|country|?', 'may|country|are', 'may|country|And', 'may|country|the', 'may|country|in', 'may|country|and', 'may|country|very', 'may|country|You', 'may|country|But', 'may|country|my', 'may|country|so', 'may|country|as', 'may|country|going', 'may|country|up', 'may|country|new', 'may|country|most', 'may|country|different', 'may|country|say']\n",
      "torch.Size([6, 100, 1024])\n",
      "torch.Size([20, 100, 1024])\n",
      "torch.Size([1, 100])\n",
      "food thinking information makes whole together four quite When before part real place ago Do technology answer looks Africa play believe everything try money become may goes anything light design getting future live sort found second even How around use where which good those only over his called went change ! Because talk day being started question give Here any example never bit off first does into thing right go way make 'm That them up What how want said The with It they it is So do at This there all an people from here should year her percent\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ceaefa2e72d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#print (' '.join([EN.vocab.itos[id.data[0]] for id in decode_str[1:]]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0manswer_token\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecode_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_kaggle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5437c4ee0c07>\u001b[0m in \u001b[0;36mevaluate_kaggle\u001b[0;34m(encoder, decoder, string, k, ngrams, max_length, batch_size)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mcurr_word_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopk_word_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mcurr_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopk_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mtemp_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcurr_word_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurr_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "match = 0 \n",
    "total = 0 \n",
    "top_match = 0\n",
    "for batch in iter(val_iter): \n",
    "    for t in range(batch.src.size()[1]): \n",
    "        string = batch.src[:,t]\n",
    "        decode_str = batch.trg[:, t]\n",
    "        #print (' '.join([DE.vocab.itos[id.data[0]] for id in string]))\n",
    "        #print (' '.join([EN.vocab.itos[id.data[0]] for id in decode_str[1:]]))\n",
    "        answer_token ='|'.join([EN.vocab.itos[id.data[0]] for id in decode_str[1:4]])\n",
    "        output_tokens = evaluate_kaggle(encoder2.cuda(), decoder2.cuda(), string, k = 100, ngrams = 3, batch_size=100).split(\" \")\n",
    "        print(answer_token)\n",
    "        print(output_tokens)\n",
    "        if answer_token in output_tokens: \n",
    "            match += 1 \n",
    "            if answer_token in output_tokens[:3]: \n",
    "                top_match += 1 \n",
    "        total += 1 \n",
    "    print(top_match/total)\n",
    "    print(match/total)\n",
    "print(\"accuracy: \", match/total)\n",
    "#64: 0.46875, 0.65625 #128 0.4609 0.633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('source_test.txt', 'r') as fp: \n",
    "    lines = fp.readlines()\n",
    "    \n",
    "print(len(lines))\n",
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")\n",
    "\n",
    "with open('sample1.txt', 'w') as fp: \n",
    "    fp.write('id,word\\n')\n",
    "    for i in range(len(lines)): \n",
    "        if (i%100 == 0): \n",
    "            print(i)\n",
    "        line = lines[i]\n",
    "        tokens = line.strip(\"\\n\").split(\" \")\n",
    "        input_index = [DE.vocab.stoi[t] for t in tokens]\n",
    "        input_index = Variable(torch.Tensor((input_index)).long().cuda())\n",
    "        output_str = evaluate_kaggle(encoder2.cuda(), decoder2.cuda(), input_index, k = 100, ngrams = 3, batch_size=100)\n",
    "        output_str = escape(output_str)\n",
    "        fp.write(str(i+1) + ',' + output_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample1.txt', 'r') as fp: \n",
    "    lines = fp.readlines()\n",
    "    with open('sample2.txt', 'w') as wp:\n",
    "        wp.write('id,word\\n')\n",
    "        for i in range(1, len(lines)): \n",
    "            line=lines[i]\n",
    "            tokens = line.split(\",\")\n",
    "            print(tokens)\n",
    "            wp.write(str(i) + ',' + tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw3-s18/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id,word\n",
    "1,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "2,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "3,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "4,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. (In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.)\n",
    "\n",
    "Finally, as always please put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
