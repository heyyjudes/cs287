{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to process the raw data using a tokenizer. We are going to be using spacy, which can be installed via:  \n",
    "  `[sudo] pip install spacy`  \n",
    "  \n",
    "Tokenizers for English/German can be installed via:  \n",
    "  `[sudo] python -m spacy download en`  \n",
    "  `[sudo] python -m spacy download de`\n",
    "  \n",
    "This isn't *strictly* necessary, and you can use your own tokenization rules if you prefer (e.g. a simple `split()` in addition to some rules to acccount for punctuation), but we recommend sticking to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to add the beginning-of-sentence token `<s>` and the end-of-sentence token `</s>` to the \n",
    "target so we know when to begin/end translating. We do not need to do this on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data. This may take a few minutes.\n",
    "\n",
    "**While this dataset of 200K sentence pairs is relatively small compared to others, it will still take some time to train. So we are going to be only working with sentences of length at most 20 for this homework. Please train only on this reduced dataset for this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary and convert the text corpus into indices. We are going to be replacing tokens that occurred less than 5 times with `<unk>` tokens, and take the rest as our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our data into batches as usual. Batching for MT is slightly tricky because source/target will be of different lengths. Fortunately, `torchtext` lets you do this by allowing you to pass in a `sort_key` function. This will minimizing the amount of padding on the source side, but since there is still some padding you will inadvertendly \"attend\" to these padding tokens. \n",
    "\n",
    "One way to get rid of padding is to pass a binary `mask` vector to your attention module so its attention score (before the softmax) is minus infinity for the padding token. Another way (which is how we do it for our projects, e.g. opennmt) is to manually sort data into batches so that each batch has exactly the same source length (this means that some batches will be less than the desired batch size, though).\n",
    "\n",
    "However, for this homework padding won't matter too much, so it's fine to ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=0,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see that the BOS/EOS token is indeed appended to the target (English) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(val_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "print(\"Target\")\n",
    "print(batch.trg)\n",
    "print(batch.src.volatile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now that we've processed the data, we are ready to begin modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trg': <torchtext.data.field.Field object at 0x7f938bf38c18>, 'src': <torchtext.data.field.Field object at 0x7f938bf38c88>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n",
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13355\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# Set up \n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de, init_token = BOS_WORD, eos_token = EOS_WORD)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS\n",
    "\n",
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))\n",
    "\n",
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(DE.vocab.stoi[\"<s>\"], DE.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "if use_cuda: \n",
    "    train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=0,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))\n",
    "else: \n",
    "    train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "     2      2      2      2      2      2      2      2      2      2      2\n",
      "  1031     75     41     25     14     30     22    477   1232     11     11\n",
      "   240     35    479      6     12     29    107     15     23    854   5007\n",
      "  6558     15    280     85     62    177     21     48     10     34     21\n",
      "    20     24    209     32      0    180      0      8      0      8      0\n",
      "   196      0     36    288    639      9      0    221     83   3825    758\n",
      "   897      0     48   4196    151   7374  13018    507      0      0    286\n",
      "    96   1970     17    572   1465   5111      0     40    377     40   1273\n",
      "    18      4      4      4      4      4      4      4      4      4      4\n",
      "     3      3      3      3      3      3      3      3      3      3      3\n",
      "\n",
      "Columns 11 to 21 \n",
      "     2      2      2      2      2      2      2      2      2      2      2\n",
      "    22     42     28     14   2946   1027     14   3011    135      0    114\n",
      "    37      0     62     21   4368      6      8     39   6321     29   1036\n",
      "    40      6    171    340     24      8      6   9360   9156    163   9412\n",
      "    74     20   1230     17    259   3560      5     16     35    812      6\n",
      "   225      7      5     74   4612    636     45    339    119     21    250\n",
      "     0    552      7    337     27    201     63     83   2684  12912    731\n",
      "  6493   1072      0    496   1322   6947    537    587    725    175   1729\n",
      "     4      4      4      4      4      4      4      4      4      4      4\n",
      "     3      3      3      3      3      3      3      3      3      3      3\n",
      "\n",
      "Columns 22 to 31 \n",
      "     2      2      2      2      2      2      2      2      2      2\n",
      "   534   1488    375     30    806     11     30     14     14    254\n",
      "   677   4407    116    215    125   1929     29     86      9      6\n",
      "   473      6    622    543    476     82     60      6   4040     72\n",
      "   146     12   1219    181    455     40    108     12   1605      5\n",
      "  9145     20    623  10172     15      5    103    717     51      8\n",
      "    47     17    147     55      5     64    714   1402     50     15\n",
      "  7712    402    992    937    332      0    115   1538    470   4011\n",
      "     4      4      4      4     18     18      4      4      4      4\n",
      "     3      3      3      3      3      3      3      3      3      3\n",
      "[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
      "\n",
      "Target\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "  209    24    42    48    14    34    10   190   190    57    57    10    41\n",
      "   26    19   493    11    16    25    75    12    12    77  3260    80  7927\n",
      "   30   514    95     6    12    91     8   133   230   257     8   120  5677\n",
      "   19    58  5573   132    73   275     0    49    68    49     0     7    11\n",
      "   74     0    94    96   143     9     9     6     0    20    60     8    30\n",
      "   35  7410     4    10   497     6     6   132    45   373   590   249     6\n",
      " 6817     4     3   199   258  1536     0   408  3445  2400    50    13   146\n",
      "   17     3     1   234   324  9209  4697     4    99   322   949    23   890\n",
      "   13     1     1   114     4     4     4     3     0     4     4     8     4\n",
      "  164     1     1    18     3     3     3     1     4     3     3   271     3\n",
      " 1003     1     1     8     1     1     1     1     3     1     1     0     1\n",
      "    9     1     1   361     1     1     1     1     1     1     1  7186     1\n",
      "  106     1     1   523     1     1     1     1     1     1     1     4     1\n",
      "   21     1     1     4     1     1     1     1     1     1     1     3     1\n",
      "    3     1     1     3     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 13 to 25 \n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "   97    14    27  7723    14  1174   634     0   511    14     0   192    14\n",
      "   25  1083    12    11    67     9  2682   678   400     8  2673    19    10\n",
      "  155     7     8     6    11     5  4409    11  1710   142     5   414   681\n",
      " 1028    47  9539  6870    32    51    28   800     5     9    37    28    19\n",
      " 6460   387     9     9    16     5   409     8    66   572    65    20    28\n",
      "  183     4  4938    46   125  5052    66  1336  6092   110     7   527     8\n",
      "    4     3     4    50     4    17  2038     5    25   166   285   694   219\n",
      "    3     1     3  2538     3   294     4  1336  1242   104     4   533     9\n",
      "    1     1     1     4     1    99     3   182     4  8552     3    17   449\n",
      "    1     1     1     3     1   666     1     4     3    56     1   386  1800\n",
      "    1     1     1     1     1     4     1     3     1     8     1     4     7\n",
      "    1     1     1     1     1     3     1     1     1  8820     1     3     0\n",
      "    1     1     1     1     1     1     1     1     1     4     1     1     4\n",
      "    1     1     1     1     1     1     1     1     1     3     1     1     3\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 26 to 31 \n",
      "    2     2     2     2     2     2\n",
      "   48    52    24    14    14    97\n",
      "   11    43    19    98     6    12\n",
      "  141  4252    43    16  4619   122\n",
      "   19     7    49    11    11    13\n",
      "   25   196   679   833    17    19\n",
      "   91    59   114   675    20    43\n",
      "    5    17    49   840   813    37\n",
      "  105  3129    20     4     4   663\n",
      "   21    21   256     3     3    44\n",
      "    3     3     4     1     1     4\n",
      "    1     1     3     1     1     3\n",
      "    1     1     1     1     1     1\n",
      "    1     1     1     1     1     1\n",
      "    1     1     1     1     1     1\n",
      "    1     1     1     1     1     1\n",
      "[torch.cuda.LongTensor of size 16x32 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "print(\"Target\")\n",
    "print(batch.trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 2\n",
    "EOS_token = 1\n",
    "teacher_forcing_ratio = 0.5\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, nlayers=1, dropout=0):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.num_layers = nlayers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, dropout=dropout, num_layers=nlayers)\n",
    "\n",
    "    def forward(self, input_src, hidden):\n",
    "        embedded = self.embed(input_src)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden \n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)), \n",
    "                  Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size))) \n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)).cuda(), \n",
    "                  Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)).cuda()) \n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size, nlayers=1, dropout=0):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, dropout=dropout, num_layers=nlayers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(self.dropout(output))\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LEN):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    batch_size = target_variable.size()[1]\n",
    "    loss = 0 \n",
    "    if batch_size != 32: \n",
    "        return loss \n",
    "    \n",
    "    _, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    decoder_input = Variable(torch.ones(1, batch_size).long())*SOS_token\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    #use_teacher_forcing = False\n",
    "    if use_teacher_forcing: \n",
    "        for t in range(target_length):\n",
    "            decoder_output, encoder_hidden = decoder(decoder_input, encoder_hidden)\n",
    "            topv, topi = torch.max(decoder_output, 2)\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            loss += criterion(decoder_output.squeeze(0), target_variable[t])/target_length\n",
    "    else: \n",
    "        for t in range(target_length): \n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = torch.max(decoder_output, 2)\n",
    "            decoder_input = topi\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            loss += criterion(decoder_output.squeeze(0), target_variable[t])/target_length\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), 5.0)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 5.0)\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0]\n",
    "\n",
    "def validate(encoder, decoder, val_iter, criterion, max_length=MAX_LEN):\n",
    "    total_loss = 0 \n",
    "    num_batches = 0 \n",
    "    for batch in iter(val_iter): \n",
    "        \n",
    "        target_length = batch.trg.size()[0]\n",
    "        batch_length = batch.src.size()[1]\n",
    "        if batch_length != 32: \n",
    "            break \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        _, encoder_hidden = encoder(batch.src, encoder_hidden)\n",
    "        decoder_input = Variable(torch.ones(1, batch_length).long()*SOS_token)\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        loss = 0 \n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = torch.max(decoder_output, 2)\n",
    "            decoded_words.append(topi)\n",
    "            decoder_input = topi\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            loss += criterion(decoder_output.squeeze(0), batch.trg[di])\n",
    "            \n",
    "        total_loss += loss.data[0]/target_length\n",
    "        num_batches += 1 \n",
    "    return total_loss/num_batches\n",
    "\n",
    "def trainIters(encoder, decoder, train_iter, val_iter, learning_rate=0.7, num_epochs=20):\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss(ignore_index=1)\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        batch_len = 0 \n",
    "        total_loss = 0 \n",
    "        for batch in iter(train_iter): \n",
    "            loss = train(batch.src, batch.trg, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            total_loss += loss \n",
    "            batch_len += 1 \n",
    "        train_loss = total_loss/batch_len \n",
    "        print(\"train loss: \", train_loss)\n",
    "        val_loss = validate(encoder, decoder, val_iter, criterion)\n",
    "        print(\"val loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bcf988fe738e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder1' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "val_loss = validate(encoder1, decoder1, val_iter, criterion)\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  5.1401722264289855\n",
      "val loss:  4.554185762536953\n",
      "train loss:  4.517539573192597\n",
      "val loss:  4.4415394438440075\n",
      "train loss:  4.4238322784900665\n",
      "val loss:  4.436329421983717\n",
      "train loss:  4.356059159040451\n",
      "val loss:  4.402336625179098\n",
      "train loss:  4.282599523305893\n",
      "val loss:  4.327827604810213\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderLSTM(len(DE.vocab), hidden_size, batch_size=32, dropout=0.3, nlayers=2)\n",
    "decoder1 = DecoderLSTM(hidden_size, len(EN.vocab), batch_size=32, dropout=0.3, nlayers=2)\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    decoder1 = decoder1.cuda()\n",
    "trainIters(encoder1, decoder1, list(train_iter)[:500], val_iter, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 12 \n",
       "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
       "   14    14    14    14    14    14    14    14    14    14    24    10    14\n",
       "    5    15     5     5     5     5    12    12     5     6     5     5     5\n",
       "    6    11     5     6     5     6     5     5    15     5     5    11     5\n",
       "    6     5     6     5     5     6     5     5     0     5     5     6     6\n",
       "    6     6     6     6     6     5     5     5     6     6     6     6     5\n",
       "    5     6     5     0     0     5     0     5     6     5     6     0     0\n",
       "    5     5     6     5     0     6     6     0     0     6     6     5     5\n",
       "    5    18     6     5     5     6     0     6     5     0     5     6    18\n",
       "    0     4     0     0     4     4     4     6     6     0    18     0     0\n",
       "    4     4     4     4     4     4     4     4     4     4     4     4     4\n",
       "    4     3     4     4     3     3     4     4     4     4     4     4     4\n",
       "    3     3     3     3     4     4     3     3     3     4     3     3     3\n",
       "    3     3     3     3     3     3     3     3     4     3     3     3     3\n",
       "    3     4     3     4     3     3     3     3     3     3     3     3     3\n",
       "    3     3     3     3     3     3     3     3     3     3     3     3     3\n",
       "    4     3     3     3     3     3     3     3     3     3     3     4     4\n",
       "    3     3     3     3     3     3     3     3     3     4     3     3     3\n",
       "    3     3     3     3     3     3     3     4     3     3     3     3     4\n",
       "\n",
       "Columns 13 to 25 \n",
       "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
       "   14    14    14    14     0    14    24    24    14    14    42    14    14\n",
       "    5     5    12     5    12     5    12     5    11    15     5    12    12\n",
       "    5    11     6     6     5    15     6     5    15     5     5     5    15\n",
       "    6     5     6     6     6     5     0     5    15     5     5     6     5\n",
       "    6     5     5     5     5     5     5     5     5     6     5     6     6\n",
       "    5     6     5     5     6     6     0     6     6     0     6     0     0\n",
       "    6     0     6     5     5     6     5     5     6     5     6     4     5\n",
       "   18     0     6     5     0     6     5     9     6    18     0     6    18\n",
       "    6     0     0     5     4     0    18     6     0     6     4     0     6\n",
       "    4     4     4     4     4     4     6     4     4     4     4     4     4\n",
       "    4     4     4     4     4     4     4     4     4     3     3     3     4\n",
       "    4     3     3     4     3     3     4     3     3     4     3     3     3\n",
       "    3     4     3     3     3     3     3     3     4     3     3     3     3\n",
       "    3     3     3     3     3     3     3     3     3     3     3     3     3\n",
       "    3     3     3     3     3     4     3     3     3     3     3     3     3\n",
       "    3     3     3     3     3     3     3     4     3     3     3     4     4\n",
       "    3     3     4     3     3     4     4     3     3     3     3     3     3\n",
       "    4     3     3     3     4     4     3     3     3     3     3     3     3\n",
       "\n",
       "Columns 26 to 31 \n",
       "    2     2     2     2     2     2\n",
       "   24    14    14    10    14    14\n",
       "    5    12    12    12     5    12\n",
       "    6     5     5     5     5     6\n",
       "    5     5     0     6     5     5\n",
       "    6     0     6     6     6     6\n",
       "    5     5     5     5     6     6\n",
       "    5     6     5     5     5     6\n",
       "    0     5     0     4     4     4\n",
       "    4     4     6     4     0     4\n",
       "    4     4     4     4     4     3\n",
       "    3     3     4     3     3     3\n",
       "    4     3     3     3     3     3\n",
       "    3     3     3     3     3     3\n",
       "    3     3     3     3     3     3\n",
       "    3     3     3     3     3     3\n",
       "    3     4     3     3     4     3\n",
       "    3     3     3     3     3     4\n",
       "    3     3     3     3     3     3\n",
       "[torch.cuda.LongTensor of size 19x32 (GPU 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, batch, max_length=MAX_LEN):\n",
    "\n",
    "    target_length = batch.trg.size()[0]\n",
    "    batch_length = batch.src.size()[1]\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    _, encoder_hidden = encoder(batch.src, encoder_hidden)\n",
    "    decoder_input = Variable(torch.ones(1, batch_length).long()*SOS_token)\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "\n",
    "    decoded_words = []\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = torch.max(decoder_output, 2)\n",
    "        decoded_words.append(topi)\n",
    "        decoder_input = topi\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            \n",
    "    return torch.cat(decoded_words)\n",
    "\n",
    "batch = list(iter(val_iter))[8]\n",
    "# print(\"Source\")\n",
    "# print(batch.src)\n",
    "# print(\"Target\")\n",
    "# print(batch.trg)\n",
    "evaluate(encoder1, decoder1, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw3-s18/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id,word\n",
    "1,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "2,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "3,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "4,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. (In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.)\n",
    "\n",
    "Finally, as always please put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
