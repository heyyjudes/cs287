{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to process the raw data using a tokenizer. We are going to be using spacy, which can be installed via:  \n",
    "  `[sudo] pip install spacy`  \n",
    "  \n",
    "Tokenizers for English/German can be installed via:  \n",
    "  `[sudo] python -m spacy download en`  \n",
    "  `[sudo] python -m spacy download de`\n",
    "  \n",
    "This isn't *strictly* necessary, and you can use your own tokenization rules if you prefer (e.g. a simple `split()` in addition to some rules to acccount for punctuation), but we recommend sticking to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to add the beginning-of-sentence token `<s>` and the end-of-sentence token `</s>` to the \n",
    "target so we know when to begin/end translating. We do not need to do this on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data. This may take a few minutes.\n",
    "\n",
    "**While this dataset of 200K sentence pairs is relatively small compared to others, it will still take some time to train. So we are going to be only working with sentences of length at most 20 for this homework. Please train only on this reduced dataset for this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary and convert the text corpus into indices. We are going to be replacing tokens that occurred less than 5 times with `<unk>` tokens, and take the rest as our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our data into batches as usual. Batching for MT is slightly tricky because source/target will be of different lengths. Fortunately, `torchtext` lets you do this by allowing you to pass in a `sort_key` function. This will minimizing the amount of padding on the source side, but since there is still some padding you will inadvertendly \"attend\" to these padding tokens. \n",
    "\n",
    "One way to get rid of padding is to pass a binary `mask` vector to your attention module so its attention score (before the softmax) is minus infinity for the padding token. Another way (which is how we do it for our projects, e.g. opennmt) is to manually sort data into batches so that each batch has exactly the same source length (this means that some batches will be less than the desired batch size, though).\n",
    "\n",
    "However, for this homework padding won't matter too much, so it's fine to ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=0,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see that the BOS/EOS token is indeed appended to the target (English) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(val_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "print(\"Target\")\n",
    "print(batch.trg)\n",
    "print(batch.src.volatile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now that we've processed the data, we are ready to begin modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': <torchtext.data.field.Field object at 0x13d39d710>, 'trg': <torchtext.data.field.Field object at 0x131d86ac8>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n",
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13355\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# Set up \n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de, init_token = BOS_WORD, eos_token = EOS_WORD)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS\n",
    "\n",
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))\n",
    "\n",
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(DE.vocab.stoi[\"<s>\"], DE.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "if use_cuda: \n",
    "    train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=0,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))\n",
    "else: \n",
    "    train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "     2      2      2      2      2      2      2      2      2      2      2\n",
      "   152   5819     99     25    328     11     78    114     22     14     14\n",
      "  8266     26    281      6     11     56    690     74    199     63     86\n",
      "  2603     25     36     21     20      8    143    509      5    285  11243\n",
      "    21      0     58      0     17   3426      5      5     33      5     13\n",
      "     5   3340   1546      5      5    232    165      7    143     33     24\n",
      "    10    876    387     59     33    409     63     15     53    238    326\n",
      "     7      7      5    862     84     20    312   2467   1691    467   3248\n",
      "    98    509   2156     47      7   8175     43    232      9     34     10\n",
      "   249    151     23     19   3348      4    132   1299   1345    166   6437\n",
      "    16      0    126      0     32     11   2219     20      9     17     36\n",
      " 11800      0     38   2968   1203    637     39   1060   1153    564   3561\n",
      "    36     40     34     19      0    852    819     49   3938    633      0\n",
      "     4      4      4      4      4      4      4      4      4      4      4\n",
      "     3      3      3      3      3      3      3      3      3      3      3\n",
      "\n",
      "Columns 11 to 21 \n",
      "     2      2      2      2      2      2      2      2      2      2      2\n",
      "  1526    223     25     22     11    324     14   3024    628     75     22\n",
      "   759   1283      6    113     35    500    929    829    201    226    205\n",
      "     7   1474     24     26     82     37    105    171   5203     91      5\n",
      "     0     13    846     19      7   1042    557     26     29     15    841\n",
      "     5     11     27     22  11812   1934     20     11   2477      8     29\n",
      "  3854     26    789   1127     20     10     66     29     27    126   6625\n",
      "   838   1031    248    959    949  13037   1008      7     84     18      5\n",
      "  3097    240      0      5     83    131      5      0    723    178   5390\n",
      "    23     11     16      9   1695      8    346      0  13032    434      6\n",
      "    36     34     73    408      5      6     59     27  11930     15    889\n",
      "    32   1430   8009   2020     45     63   1781      0  11174     46     24\n",
      "  1916     91    287    202      0     68   3609    472    474   3358   3457\n",
      "     4     18      4      4      4      4      4      4      4     18      4\n",
      "     3      3      3      3      3      3      3      3      3      3      3\n",
      "\n",
      "Columns 22 to 31 \n",
      "     2      2      2      2      2      2      2      2      2      2\n",
      "    14     99     14    979     11     30    106     14     78     42\n",
      "   103     13     23    160     49     92    210      8   1724     15\n",
      "    48     17     56      5     76     35     21     52     66    240\n",
      "    62     74   9038     16      5      7    174     16    120     94\n",
      "    12   3292      5      9     33    451     26      9     10   2979\n",
      "  2617   3067     97   7556    117     27     25    895    396     35\n",
      "   136   1092    705      5    960     32      6   4780     82      5\n",
      "   224      5    123      6   1110      0     21   1124     24     15\n",
      "  1122    145      5      7    639   5154      0     10   1389    240\n",
      "     0     13      7   4426     70     16     38     37    428     94\n",
      "     0      0     48   2528  10590   2559   1783    707   7890   6449\n",
      "  5176   1853      0      0   9715    313   4250   6725     17     35\n",
      "     4      4      4      4      4      4      4      4      4      4\n",
      "     3      3      3      3      3      3      3      3      3      3\n",
      "[torch.LongTensor of size 15x32]\n",
      "\n",
      "Target\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "     2      2      2      2      2      2      2      2      2      2      2\n",
      "  1357   5538    167     48    250     57    116    128     10     14     14\n",
      "  5283     39     44      5     30     43     84    393     74     81     10\n",
      "  2557      0     12    139    165     37     20     13     13    356     87\n",
      "     5   4553    289     11   2834     65    130      5     12     13    102\n",
      "    18    566     33      0      0      7     81   2895    226    150     18\n",
      "     6     33      6      5     82      0   6993     66      9    353   3249\n",
      "   109      6   1026     11    749     69    260    871     32    706      8\n",
      "   199    455      5    127      4    462     56      5      6      7    131\n",
      "   158      9     31    700      3     12    169     31   1323     40   3307\n",
      "    17      8    463     56      1   3902     56     36      9    569     18\n",
      " 10923   8010     16    814      1      4    668     26   1075     83     10\n",
      "     4      0     46   2049      1     57      4     16     11    458     87\n",
      "     3      4      4      0      1    140      3    228      4      4   1018\n",
      "     1      3      3      4      1    168      1    134      3      3     29\n",
      "     1      1      1      3      1      4      1     94      1      1   1723\n",
      "     1      1      1      1      1      3      1      4      1      1      0\n",
      "     1      1      1      1      1      1      1      3      1      1      7\n",
      "     1      1      1      1      1      1      1      1      1      1    163\n",
      "     1      1      1      1      1      1      1      1      1      1      4\n",
      "     1      1      1      1      1      1      1      1      1      1      3\n",
      "     1      1      1      1      1      1      1      1      1      1      1\n",
      "\n",
      "Columns 11 to 21 \n",
      "     2      2      2      2      2      2      2      2      2      2      2\n",
      "    42    630     48     10     57    221     14     57    221     24     24\n",
      "  1084      5     11    101     84   1273      5     43      0    162     17\n",
      "     6     10      8      5     30     23     56    127     11     26     20\n",
      "   857    138    828     22    206    148      8      5     13     46    106\n",
      "  1020    327      9     10      6     18    754    304    334      9      5\n",
      "     5     15    564     62   1275   3136      5    155   3170     20     10\n",
      "     6     39    252    580      7    243   2375      5      9     21     74\n",
      "   950    209    670     15     59     81    460      6     15    209     13\n",
      "    16    177   2119   2988     99     79   1531    153     25    389    665\n",
      "  9964     15     17      7    746   4504     37   1968    648     64     25\n",
      "     7    406      8    809     32     12    146      0   3143   3462   5718\n",
      "   845     21   1868      4     23      5    737      9     35     21      5\n",
      "     4      3    324      3     17     18      5      0   3635      3     18\n",
      "     3      1      4      1      6     81     93     13    418      1    858\n",
      "     1      1      3      1    455     12   1720     19      4      1   4969\n",
      "     1      1      1      1      4    148      4     80      3      1     11\n",
      "     1      1      1      1      3     18      3    263      1      1      8\n",
      "     1      1      1      1      1   3136      1   7258      1      1   2516\n",
      "     1      1      1      1      1     91      1      4      1      1      4\n",
      "     1      1      1      1      1      4      1      3      1      1      3\n",
      "     1      1      1      1      1      3      1      1      1      1      1\n",
      "\n",
      "Columns 22 to 31 \n",
      "     2      2      2      2      2      2      2      2      2      2\n",
      "    14    167     14    227     52     34     24     14    116     41\n",
      "    19     10     31      9    239     80     67    995     54     19\n",
      "    28    102     25     68     72     46     12      5   1422     77\n",
      "  1127     29      0      5     13    372     58     20     44   2092\n",
      "  4629      8      5     60     55      6    218     11   1220      7\n",
      "  1492   1037     18   6686   7400   2151      4    334     59     28\n",
      "     8      0   1189      5    143      9     48   3128   3413   9613\n",
      "   178      5    100     11   4626      6     11      5     29    170\n",
      "     9     10     19      6    497   4561     58     18     58     19\n",
      "   901     90     43    547      6   1206   5131     16    974     43\n",
      "  2222     92   1240      9    915     17   9883     79      4   2092\n",
      "     4   8216      9    213      4   2596    835    232      3      7\n",
      "     3   2253      4      0      3      4     13     96      1     28\n",
      "     1      4      3      4      1      3     12      4      1   3767\n",
      "     1     10      1      3      1      1    784      3      1      4\n",
      "     1     26      1      1      1      1     29      1      1      3\n",
      "     1      4      1      1      1      1    512      1      1      1\n",
      "     1      3      1      1      1      1   3129      1      1      1\n",
      "     1      1      1      1      1      1      4      1      1      1\n",
      "     1      1      1      1      1      1      3      1      1      1\n",
      "     1      1      1      1      1      1      1      1      1      1\n",
      "[torch.LongTensor of size 22x32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "print(\"Target\")\n",
    "print(batch.trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 2\n",
    "EOS_token = 1\n",
    "teacher_forcing_ratio = 0.5\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_src, hidden):\n",
    "        embedded = self.embed(input_src)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden \n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = (Variable(torch.zeros(1, self.batch_size, self.hidden_size)), \n",
    "                  Variable(torch.zeros(1, self.batch_size, self.hidden_size))) \n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(1, self.batch_size, self.hidden_size)).cuda(), \n",
    "                  Variable(torch.zeros(1, self.batch_size, self.hidden_size)).cuda()) \n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = (Variable(torch.zeros(1, self.batch_size, self.hidden_size)), \n",
    "                  Variable(torch.zeros(1, self.batch_size, self.hidden_size))) \n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(1, self.batch_size, self.hidden_size)).cuda(), \n",
    "                  Variable(torch.zeros(1, self.batch_size, self.hidden_size)).cuda()) \n",
    "        else:\n",
    "            return result\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LEN):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    batch_size = target_variable.size()[1]\n",
    "    loss = 0 \n",
    "    \n",
    "    _, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    decoder_input = Variable(torch.ones(1, batch_size).long())*SOS_token\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    #use_teacher_forcing = False\n",
    "    if use_teacher_forcing: \n",
    "        for t in range(target_length):\n",
    "            decoder_output, encoder_hidden = decoder(decoder_input, encoder_hidden)\n",
    "            topv, topi = torch.max(decoder_output, 2)\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            loss += criterion(decoder_output.squeeze(0), target_variable[t])/target_length\n",
    "    else: \n",
    "        for t in range(target_length): \n",
    "            decoder_output, encoder_hidden = decoder(decoder_input, encoder_hidden)\n",
    "            topv, topi = torch.max(decoder_output, 2)\n",
    "            decoder_input = topi\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            loss += criterion(decoder_output.squeeze(0), target_variable[t])/target_length\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), 5.0)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 5.0)\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0]\n",
    "\n",
    "def validate(encoder, decoder, val_iter, criterion, max_length=MAX_LEN):\n",
    "    total_loss = 0 \n",
    "    num_batches = 0 \n",
    "    for batch in iter(val_iter): \n",
    "        \n",
    "        target_length = batch.trg.size()[0]\n",
    "        batch_length = batch.src.size()[1]\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        _, encoder_hidden = encoder(batch.src, encoder_hidden)\n",
    "        decoder_input = Variable(torch.ones(1, batch_length).long()*SOS_token)\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        loss = 0 \n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = torch.max(decoder_output, 2)\n",
    "            decoded_words.append(topi)\n",
    "            decoder_input = topi\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            loss += criterion(decoder_output.squeeze(0), batch.trg[di])\n",
    "            \n",
    "        total_loss += loss.data[0]/target_length\n",
    "        num_batches += 1 \n",
    "    return total_loss/num_batches\n",
    "\n",
    "def trainIters(encoder, decoder, train_iter, val_iter, learning_rate=0.5, num_epochs=20):\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        batch_len = 0 \n",
    "        total_loss = 0 \n",
    "        for batch in iter(train_iter): \n",
    "            loss = train(batch.src, batch.trg, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            total_loss += loss \n",
    "            batch_len += 1 \n",
    "        train_loss = total_loss/batch_len \n",
    "        print(\"train loss: \", train_loss)\n",
    "        val_loss = validate(encoder, decoder, val_iter, criterion)\n",
    "        print(\"val loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.285083382111696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:101: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "val_loss = validate(encoder1, decoder1, val_iter, criterion)\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  5.177323170900345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:101: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:  4.7093320368688225\n",
      "train loss:  4.083388266563415\n",
      "val loss:  4.23619791985956\n",
      "train loss:  3.9206043767929075\n",
      "val loss:  4.028447969912756\n",
      "train loss:  3.834988394975662\n",
      "val loss:  4.057441439603054\n",
      "train loss:  3.7863912147283556\n",
      "val loss:  4.054190803405945\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderLSTM(len(DE.vocab), hidden_size, batch_size=1)\n",
    "decoder1 = DecoderLSTM(hidden_size, len(EN.vocab), batch_size=1)\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    decoder1 = decoder1.cuda()\n",
    "trainIters(encoder1, decoder1, train_iter, val_iter, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 12 \n",
       "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    5     0     5     0     0     0     0     0     0     0     5     5     5\n",
       "    0     5     0     5     5     5     5     5     5     5     0     0     0\n",
       "    5     0     5     0     0     0     0     0     0     0     5     5     5\n",
       "    0     5     0     5     5     5     5     5     5     5     0     0     0\n",
       "    5     0     5     0     0     0     0     0     0     0     5     5     5\n",
       "    0     5     0     5     5     5     5     5     5     5     0     0     0\n",
       "    4     0     4     0     0     0     0     0     0     0     5     5     5\n",
       "    4     4     4     4     4     4     4     0     4     4     4     0     0\n",
       "    3     4     3     4     4     4     4     4     4     4     4     4     4\n",
       "    4     3     4     3     3     3     4     4     4     4     3     4     3\n",
       "    3     4     3     4     4     4     3     3     3     3     4     3     4\n",
       "    1     3     1     3     3     3     4     4     4     3     3     4     3\n",
       "    1     1     1     3     4     4     3     3     3     4     3     3     1\n",
       "    1     1     1     1     3     1     1     4     1     3     1     1     1\n",
       "    1     1     1     1     1     1     1     3     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 13 to 25 \n",
       "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     5     5     0     0     0     5     0     5     5     5     5     5\n",
       "    5     0     0     5     5     5     0     5     0     0     0     0     0\n",
       "    0     5     5     0     0     0     5     0     5     5     5     5     5\n",
       "    5     0     0     5     5     5     0     5     0     0     0     0     0\n",
       "    0     5     5     0     0     0     5     0     5     5     5     5     5\n",
       "    5     0     0     5     5     5     0     5     0     0     0     0     0\n",
       "    0     4     5     0     0     0     5     0     4     5     4     4     5\n",
       "    0     4     0     0     4     0     0     4     4     4     3     4     4\n",
       "    4     3     4     4     4     4     5     4     3     4     4     3     4\n",
       "    4     4     3     4     3     4     4     4     4     3     3     4     3\n",
       "    3     3     4     3     3     3     3     3     3     3     1     3     4\n",
       "    4     3     1     4     4     4     4     4     1     1     1     1     3\n",
       "    3     1     1     3     1     3     1     3     1     1     1     1     1\n",
       "    4     1     1     3     1     3     3     3     1     1     1     1     1\n",
       "    3     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     3     1     1     1     1     1     1\n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 26 to 31 \n",
       "    2     2     2     2     2     2\n",
       "    0     0     0     0     0     0\n",
       "    5     5     5     5     5     5\n",
       "    0     0     0     0     0     0\n",
       "    5     5     5     5     5     5\n",
       "    0     0     0     0     0     0\n",
       "    5     5     5     5     5     5\n",
       "    0     0     0     0     0     0\n",
       "    5     4     4     5     5     5\n",
       "    4     3     3     4     0     4\n",
       "    4     4     4     4     4     4\n",
       "    3     3     3     3     3     3\n",
       "    4     1     4     4     4     4\n",
       "    3     1     1     3     3     1\n",
       "    1     1     1     1     4     1\n",
       "    1     1     1     1     1     1\n",
       "    1     1     1     1     1     1\n",
       "    1     1     1     1     1     1\n",
       "    1     1     1     1     1     1\n",
       "[torch.LongTensor of size 19x32]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, batch, max_length=MAX_LEN):\n",
    "\n",
    "    target_length = batch.trg.size()[0]\n",
    "    batch_length = batch.src.size()[1]\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    _, encoder_hidden = encoder(batch.src, encoder_hidden)\n",
    "    decoder_input = Variable(torch.ones(1, batch_length).long()*SOS_token)\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "\n",
    "    decoded_words = []\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = torch.max(decoder_output, 2)\n",
    "        decoded_words.append(topi)\n",
    "        decoder_input = topi\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            \n",
    "    return torch.cat(decoded_words)\n",
    "\n",
    "batch = list(iter(val_iter))[8]\n",
    "# print(\"Source\")\n",
    "# print(batch.src)\n",
    "# print(\"Target\")\n",
    "# print(batch.trg)\n",
    "evaluate(encoder1, decoder1, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw3-s18/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id,word\n",
    "1,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "2,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "3,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "4,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. (In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.)\n",
    "\n",
    "Finally, as always please put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
